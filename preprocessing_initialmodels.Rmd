---
title: "Identifying Risk Factors and Preferred Hospitals for Hip/Knee Replacements: An Analysis of 2019-2022 Hospital Readmission Reduction Program Data"
subtitle: "Team Iota: Preprocessing and Inital Model Report"
author: "Week Five Team Lead: Scott Eugley |  Week Five Recorder/Spokesperson: Adeline Casali"
date: "2024-07-31"
output:
  html_document:
    toc: true
    toc_float: true
    theme: flatly
    fig_crop: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = FALSE,
                      cache.comments = TRUE,
                      size = 13)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# clean, set up, and load
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
rm(list = ls(all = TRUE))

pacman::p_load(tidyverse, 
               ggplot2, 
               kableExtra, 
               lubridate, 
               caret,
               janitor,
               naniar, 
               maps, 
               psych, 
               reshape2, 
               corrplot,
               VIM,
               glmnet, 
               psych,  
               reshape2) 

load("HipKneeClean.Rdata")
load("HipKneeTrain.Rdata")
load("HipKneeTest.Rdata")
```

[GitHub Repository Here](https://github.com/adelinecasali4/hospital-readmission/tree/main)

# Background and Question
### Question  
What are the most significant risk factors associated with hospital readmission rate in hip/knee replacement patients?  

### Hypothesis  
Hospitals with better Hospital Consumer Assessment of Healthcare Providers and Systems (HCAHPS) scores will have lower readmission rates for hip/knee replacements because higher patient satisfaction often correlates with better overall care quality and patient outcomes, including reduced complications and better post-discharge support (Edwards et al., 2015).  

### Prediction  
It is anticipated that patient satisfaction, clinical treatment quality, and operational efficiency will be the most important risk factor categories associated with hospital readmission rates among patients who have had hip or knee replacement surgery. The most important variables influencing readmission rates for patients after hip or knee replacement surgery will most likely be a combination of high HCAHPS scores and reduced rates of clinical complications.  

# Methods
Our preprocessing and feature engineering approach was designed to address the complexities highlighted during exploratory data analysis (EDA). The initial plan involved several key steps:

Preprocessing: We focused on handling missing values, encoding categorical variables, and reducing collinearity. The preprocessing steps included dummy encoding of ordinal data, mapping categorical states to numeric codes, and removing variables with high missingness or redundancy. This approach aimed to ensure that the dataset was clean and suitable for further analysis. We specifically addressed the high number of missing values and collinearity issues as identified in our EDA.

Feature Engineering: We engineered features to consolidate related data and reduce dimensionality. For instance, we created a composite variable for overall mortality scores and removed redundant or highly correlated variables. This strategy aimed to simplify the feature set while retaining critical information, thus enhancing model performance and interpretability.

Initial Model Selection: Based on our EDA findings, we planned to explore multiple methods of unsupervised learning and employ a random forest model. Unsupervised methods, such as KNN for segmentation analysis, were chosen to identify clusters and uncover underlying patterns in the data. The random forest model was selected as an initial supervised learning approach due to its robustness in handling diverse data types and managing feature importance.

## Preprocessing and Feature Selection
To ensure the dataset's suitability for analysis, several preprocessing steps were employed. First, categorical variables were identified, and appropriate encoding methods were applied. The EDV column, which contains ordinal data, was dummy encoded based on predefined levels, transforming it into a numeric format that reflects its ordinal nature. This approach preserves the inherent order of the data while making it suitable for statistical modeling. Additionally, categorical states were encoded using a numeric mapping to eliminate potential ordinal relationships and standardize the representation of states. This choice was made to facilitate model compatibility and avoid any unintended ordinal implications in the analysis.

Unnecessary or highly correlated variables were then removed to streamline the dataset. Variables with significant missingness or redundancy, such as certain patient survey ratings and summary statistics, were excluded. The decision to remove these variables was guided by their high correlation with other variables, high missingness, or redundancy with already included metrics. For instance, combining various patient survey metrics into a single comprehensive score helped reduce collinearity and simplify the feature set. See table 1 for a list of all variables that were removed and justification for doing so. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Create a data frame with the variable names and justifications
removed_vars <- data.frame(
  Variable = c("OP_18c", "OP_22", "ED_2_Strata_1", "OP_23", "VTE_2", 
               "STK_02", "STK_05", "STK_06", "HcahpsLinearMeanValue_H_RECMND_LINEAR_SCORE", 
               "ExcessReadmissionRatio_HIP-KNEE", "ExpectedReadmissionRate_HIP-KNEE", 
               "NumberOfReadmissions_HIP-KNEE", "Sepsis Variables", "Score_PSI_90",
               "Patient Survey Data"),
  Justification = c("Removed due to high correlation and low relevance.",
                     "Removed due to high correlation and low relevance.",
                     "Removed due to high percentage of missingness.",
                     "Removed due to high percentage of missingness.",
                     "Removed due to high percentage of missingness.",
                     "Removed as stroke data is not relevant to Hip/Knee Surgery.",
                     "Removed as stroke data is not relevant to Hip/Knee Surgery.",
                     "Removed as stroke data is not relevant to Hip/Knee Surgery.",
                     "Removed due to strong correlation with overall hospital rating.",
                     "Removed due to high correlation with target variable.",
                     "Removed due to high correlation with target variable.",
                     "Removed as it is influenced by hospital size, which is not available.",
                     "Removed due to unclear definition in dataset dictionary.",
                     "Removed because it is a summary of other PSI variables, making it redundant.",
                     "Removed due to collinearity and redundancy with other patient survey metrics.")
)

# Print the table
removed_vars %>%
  kable(caption = "Table 1: Justifications for Removal of Variables") %>%
  kable_styling(bootstrap_options = c("hover", "striped", "responsive"))
```

Feature engineering focused on creating meaningful variables from existing data and removing irrelevant or redundant features. A notable example is the creation of the Score_Ovr_MORT variable, which aggregates multiple mortality-related scores into a single measure. This new feature consolidates related variables, reducing dimensionality while preserving critical information on mortality rates.

## Initial Model
Make sure to briefly recap why you chose this as your initial model if you didn't earlier
(1-2 paragraphs) What methods did you use to perform your initial model? Why did you make the choices that you did?
Did you do cross-validation? If yes, describe.
What are your model's assumptions? Did you test for them? If yes, describe.

# Results
## Preprocessing and Feature Engineering
First, categorical variables were appropriately encoded, and redundant variables were removed, as described in the previous section. Collinearity was greatly reduced, as evident by the heat maps in Figure 1 and Figure 2. Additionally, all rows with NA values for the target variables were removed. We also decided to remove the one facility that had an NA value, which happened to be the same observation with a missing state value. Variables with less than 5% missing data were imputed by the median, as this has a small impact on the variation of the predictors. Any variables with greater than 5% missing values were imputed using kNN to allow for more accurate imputation and mitigate reduction in variation. All steps were repeated on the test dataset, which is the most recent snapshot of the data from April 24, 2024. Finally, descriptive statistics were computed for all variables (Table 2, Figure 3). 

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=14, fig.height=12}
# Compute correlation matrix
cor_matrix <- cor(HipKneeClean %>% select_if(is.numeric), use = "pairwise.complete.obs")

# Melt the correlation matrix into a long format
cor_melted <- melt(cor_matrix)

# Plot heatmap
ggplot(cor_melted, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Figure 1. Correlation Heatmap of Numeric Variables Before Preprocessing")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=14, fig.height=12}
# Compute correlation matrix
cor_matrix <- cor(HipKneeTrain %>% select_if(is.numeric), use = "pairwise.complete.obs")

# Melt the correlation matrix into a long format
cor_melted <- melt(cor_matrix)

# Plot heatmap
ggplot(cor_melted, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Figure 2. Correlation Heatmap of Numeric Variables After Preprocessing")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Create a summary table of descriptive statistics
descr_stats <- describe(HipKneeTrain)
# Remove the rows with Facility ID, State and State code, and facility name
descr_stats <- descr_stats %>% filter(vars != c(1, 23, 24, 26))
# Remove columns 1, 2, 5, and 6
descr_stats <- descr_stats[, -c(1, 2, 5, 6)]

# Create a table with kable
kable(descr_stats, format = "html", caption = "Table 2. Descriptive Statistics for All Numeric Variables in Final Dataset") %>%
  kable_styling(
    bootstrap_options = c("hover", "striped", "responsive")
  ) %>%
  column_spec(1, bold = TRUE) %>%
  column_spec(2, width = "5em") %>%
  row_spec(0, bold = TRUE, background = "#f2f2f2")

# Select numeric columns
numeric_columns <- HipKneeTrain %>% select_if(is.numeric)

# Melt the data for easier plotting with ggplot2
numeric_melted <- melt(numeric_columns)

# Create histograms
ggplot(numeric_melted, aes(x = value)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  facet_wrap(~variable, scales = "free_x") +
  theme_minimal() +
  labs(title = "Figure 3. Histograms of Numeric Variables", x = "Value", y = "Frequency")
```


## Segmentation Analysis
NOTE: Make sure you thoroughly describe your unsupervised ML methods.
Note: If inappropriate to your data, justification for exclusion may be provided instead.

## Initial Model 
Give your analysis of your initial model's (1) algorithm, (2) assumptions, and (3) overfitting for your initial model, using tables, graphs, and statistics to defend choices and interpretations, and do not count toward the page limit.
Algorithm: appropriate tables or figures are given to interpret the preliminary model results using evaluation metrics
Assumptions: include any tests (e.g., Durbin-Watson), graphs (e.g., residuals plots), heuristics, or other diagnostics as needed
Overfitting: control for possible overfitting, which may include but is not limited to: (1) validation with the training and testing sets, (2) cross-validation, (3) regularization or penalties, and/or (4) additional feature selection
This will be performed in greater detail next week, so you may just give a cursory treatment to this to save you time this week. (~ 1 paragraph)

# Discussion and Next Steps
(1-2 paragraphs) Include a summary of your key takeaways from both the Pre-processing & feature selection, as well as the initial model you ran
Make sure to refer back to the question you've proposed
Refer back to what you initially proposed and revise if needed! 
Include justifications for changes, including referring back to sections, tables, or graphs in your text.
(1-2 paragraphs) Includes a plan for next steps: Model Selection & Tuning Plan
What additional models will you run? How will you validate them?
What hyperparameter tuning is needed?
Did this analysis change anything in your Analysis Plan?

# Appendix 1: Data Dictionary
```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Define the data dictionary
data_dictionary <- data.frame(
  Measure_ID = c("FacilityId*", "FacilityName", "State", "ExcessReadmissionRatio_HIP-KNEE", 
                 "PredictedReadmissionRate_HIP_KNEE*", "ExpectedReadmissionRate_HIP-KNEE", 
                 "NumberOfReadmissions_HIP-KNEE", "H_HSP_RATING_LINEAR_SCORE*", 
                 "H_RECMND_LINEAR_SCORE", "EDV*", "ED_2", "IMM_3*", "HCP_COVID_19*", 
                 "OP_18b*", "OP_18c", "OP_22", "OP_23", "OP_29*", "SAFE_USE_OF_OPIOIDS*", 
                 "SEP_1", "SEP_SH_3HR", "SEP_SH_6HR", "SEV_SEP_3HR", "SEV_SEP_6HR", 
                 "STK_02", "STK_05", "STK_06", "VTE_1*", "VTE_2", "Score_COMP_HIP_KNEE*", 
                 "Score_MORT_30_AMI", "Score_MORT_30_COPD", "Score_MORT_30_HF", 
                 "Score_MORT_30_PN", "Score_MORT_30_STK", "Score_Ovr_MORT*", 
                 "Score_PSI_03*", "Score_PSI_04*", "Score_PSI_06*", "Score_PSI_08*", 
                 "Score_PSI_09*", "Score_PSI_10*", "Score_PSI_11*", "Score_PSI_12*", 
                 "Score_PSI_13*", "Score_PSI_14*", "Score_PSI_15*", "Score_PSI_90", 
                 "Payment_PAYM_90_HIP_KNEE*"),
  Description = c("Unique facility identifier.", "Name of the facility.", "State where the facility is located.", 
                  "The ratio of the predicted readmission rate to the expected readmission rate, based on an average hospital with similar patients. Performance is compared against a ratio of one, such that below one is better and above one is worse in terms of readmission rates.", 
                  "The number of readmissions within 30 days predicted based on the hospital’s performance with its observed case mix. The predicted number of readmissions is estimated using a hospital-specific intercept, and is intended to reflect the annual expected performance of the hospital given its historical case and patient mix and performance.", 
                  "The expected number of readmissions in each hospital is estimated using its patient mix and an average hospital-specific intercept. It is thus indirectly standardized to other hospitals with similar case and patient mixes.", 
                  "Crude number of readmissions in each hospital within 30 days.", 
                  "Overall hospital rating - linear mean score. Employs all survey response items in each HCAHPS measure and are converted and combined into a 0-100 linear-scaled measure score.", 
                  "Recommend hospital - linear mean score. From question: Would you recommend this hospital to your friends and family?", 
                  "Emergency department volume. Number based on the volume of patients submitted by a hospital used for the measure OP-22: Left without Being Seen.", 
                  "Average (median) admit decision time to time of departure from the emergency department for emergency department patients admitted to inpatient status.", 
                  "Healthcare workers given influenza vaccination.", 
                  "COVID-19 vaccination coverage among healthcare providers.", 
                  "Average (median) time patients spent in the emergency department before leaving from the visit.", 
                  "Average time patients spent in the emergency department before being sent home (Median Time from ED Arrival to ED Departure for Discharged ED Patients – Psychiatric/Mental Health Patients).", 
                  "Percentage of patients who left the emergency department before being seen.", 
                  "Percentage of patients who came to the emergency department with stroke symptoms who received brain scan results within 45 minutes of arrival.", 
                  "Percentage of patients receiving appropriate recommendation for follow-up screening colonoscopy.", 
                  "Percentage of patients who were prescribed 2 or more opioids or an opioid and benzodiazepine concurrently at discharge.", 
                  "Severe sepsis and septic shock.", 
                  "Septic shock 3 hour.", 
                  "Septic shock 6 hour.", 
                  "Severe sepsis 3 hour.", 
                  "Severe sepsis 6 hour.", 
                  "Percentage of ischemic stroke patients prescribed or continuing to take antithrombotic therapy at hospital discharge.", 
                  "Percentage of ischemic stroke patients administered antithrombotic therapy by the end of hospital day 2.", 
                  "Percentage of ischemic stroke patients who are prescribed or continuing to take statin medication at hospital discharge.", 
                  "Percentage of patients that received VTE prophylaxis after hospital admission or surgery.", 
                  "Percentage of patients that received VTE prophylaxis after being admitted to the intensive care unit (ICU).", 
                  "Rate of complications for hip/knee replacement patients.", 
                  "Death rate for heart attack patients.", 
                  "Death rate for chronic obstructive pulmonary disease (COPD) patients.", 
                  "Death rate for heart failure patients.", 
                  "Death rate for pneumonia patients.", 
                  "Death rate for stroke patients.", 
                  "Summary measure (row-wise mean) of Score_MORT_30_AMI, Score_MORT_30_COPD, Score_MORT_30_HF, Score_MORT_30_PN, and Score_MORT_30_STK.", 
                  "Rate of pressure sores.", 
                  "Deaths among patients with serious treatable complications after surgery.", 
                  "Collapsed lung due to medical treatment.", 
                  "Broken hip from a fall after surgery.", 
                  "Postoperative hemorrhage or hematoma rate.", 
                  "Kidney and diabetic complications after surgery.", 
                  "Respiratory failure after surgery.", 
                  "Serious blood clots after surgery.", 
                  "Blood stream infection after surgery.", 
                  "A wound that splits open after surgery on the abdomen or pelvis.", 
                  "Accidental cuts and tears from medical treatment.", 
                  "Serious complications (this is a composite or summary measure).", 
                  "Payment for hip/knee replacement - estimates of payments associated with a 90-day episode of care for hip/knee replacement.")
)

# Create the table
kable(data_dictionary, caption = "Data Dictionary<br><i>* indicates measure utilized in final dataset</i>") %>%
  kable_styling(full_width = F, position = "center", bootstrap_options = c("striped", "hover", "condensed"))
```

# Appendix 2: Preprocessing and Feature Engineering Code

## Variable Encoding (SE)

#### Identify Categorical Variables
```{r}
# Create function to find categorical variables
is_categorical <- function(x) is.factor(x) | is.character(x)

# Apply function to all variables in the dataset
categorical_vars <- sapply(HipKneeClean, is_categorical)

# Print the names of all categorical variables
categorical <- names(HipKneeClean)[categorical_vars]
categorical
```

#### Dummy encode the EDV column 
```{r}
# Define the encoding mapping (ignore NAs for now)
encoding_map <- c(
  'low' = 1,
  'medium' = 2,
  'high' = 3,
  'very high' = 4
)
# Dummy encoding used due to ordinal nature of this data

# Create a copy of HipKneeClean and name it HipKneeTrain to separate cleaned dataset and the training dataset
HipKneeTrain <- HipKneeClean %>%
  mutate(EDV = recode(EDV, !!!encoding_map))

# Print first 20 rows of EDV column in HipKneeClean and HipKneeTrain to ensure proper encoding
cat("HipKneeClean")
print(head(HipKneeClean$EDV, 20))

cat("HipKneeTrain")
print(head(HipKneeTrain$EDV, 20))
```

#### Encode each state in alphabetical order
```{r}
# Manually map out each state with their respective code in alphabetical order with a preceding 0 to make data non-ordinal
state_mapping <- c(
  "AL" = "001",
  "AK" = "002",
  "AZ" = "003",
  "AR" = "004",
  "CA" = "005",
  "CO" = "006",
  "CT" = "007",
  "DE" = "008",
  "FL" = "009",
  "GA" = "010",
  "HI" = "011",
  "ID" = "012",
  "IL" = "013",
  "IN" = "014",
  "IA" = "015",
  "KS" = "016",
  "KY" = "017",
  "LA" = "018",
  "ME" = "019",
  "MD" = "020",
  "MA" = "021",
  "MI" = "022",
  "MN" = "023",
  "MS" = "024",
  "MO" = "025",
  "MT" = "026",
  "NE" = "027",
  "NV" = "028",
  "NH" = "029",
  "NJ" = "030",
  "NM" = "031",
  "NY" = "032",
  "NC" = "033",
  "ND" = "034",
  "OH" = "035",
  "OK" = "036",
  "OR" = "037",
  "PA" = "038",
  "RI" = "039",
  "SC" = "040",
  "SD" = "041",
  "TN" = "042",
  "TX" = "043",
  "UT" = "044",
  "VT" = "045",
  "VA" = "046",
  "WA" = "047",
  "WV" = "048",
  "WI" = "049",
  "WY" = "050"
)

# Create new "StateCode" column with the encoded values
HipKneeTrain <- HipKneeTrain %>%
  mutate(StateCode = state_mapping[State])

# Print 100 rows of the "State" and "StateCode" columns to ensure accuracy
print("State and StateCode Columns")
print(head(HipKneeTrain[c("State", "StateCode")], 100))

# Print all unique values in "StateCode" column to ensure accuracy
print("Unique StateCode Values")
print(unique(HipKneeTrain$StateCode))
```

## Collinearity and Feature Removal (SE)

#### Remove correlated and unnecessary variables 
```{r}
# Specify columns to remove
columns_to_remove <- c(
  "ED_2_Strata_1",
  "OP_23",
  "VTE_2",
  "OP_18c",
  "OP_22",
  "STK_02",
  "STK_05",
  "STK_06",
  "HcahpsLinearMeanValue_H_RECMND_LINEAR_SCORE",
  "NumberOfReadmissions_HIP-KNEE",
  "ExcessReadmissionRatio_HIP-KNEE",
  "ExpectedReadmissionRate_HIP-KNEE",
  "SEP_1",
  "SEV_SEP_6HR",
  "SEV_SEP_3HR",
  "SEP_SH_6HR",
  "SEP_SH_3HR",
  "Score_PSI_90",
  "PatientSurveyStarRating_H_COMP_1_STAR_RATING",    
  "PatientSurveyStarRating_H_COMP_2_STAR_RATING",    
  "PatientSurveyStarRating_H_COMP_3_STAR_RATING",    
  "PatientSurveyStarRating_H_COMP_5_STAR_RATING",    
  "PatientSurveyStarRating_H_COMP_6_STAR_RATING",    
  "PatientSurveyStarRating_H_COMP_7_STAR_RATING",    
  "PatientSurveyStarRating_H_CLEAN_STAR_RATING",     
  "PatientSurveyStarRating_H_QUIET_STAR_RATING",     
  "PatientSurveyStarRating_H_HSP_RATING_STAR_RATING",
  "PatientSurveyStarRating_H_RECMND_STAR_RATING",    
  "PatientSurveyStarRating_H_STAR_RATING",
  "HcahpsLinearMeanValue_H_COMP_1_LINEAR_SCORE",    
  "HcahpsLinearMeanValue_H_COMP_2_LINEAR_SCORE",    
  "HcahpsLinearMeanValue_H_COMP_3_LINEAR_SCORE",    
  "HcahpsLinearMeanValue_H_COMP_5_LINEAR_SCORE",    
  "HcahpsLinearMeanValue_H_COMP_6_LINEAR_SCORE",    
  "HcahpsLinearMeanValue_H_COMP_7_LINEAR_SCORE",    
  "HcahpsLinearMeanValue_H_CLEAN_LINEAR_SCORE",     
  "HcahpsLinearMeanValue_H_QUIET_LINEAR_SCORE"
)

# Remove specified columns
HipKneeTrain <- HipKneeTrain %>% select(-all_of(columns_to_remove))
```

```{r}
# Print column names to verify
print("Remaining columns:")
print(colnames(HipKneeTrain))
```
> "OP_18c" and "OP_22" removed due to being highly correlated and low relevance.
> "ED_2_Strata_1", "OP_23", and "VTE_2" removed due to high percentage of missingness. 
> "STK_02", "STK_05", and "STK_06" variables removed as we do not see stroke data as being relevant towards Hip/Knee Surgery.
> "HcahpsLinearMeanValue_H_RECMND_LINEAR_ SCORE" removed as it is strongly correlated with overall hospital rating.
> "ExcessReadmissionRatio_HIP-KNEE", and "ExpectedReadmissionRate_HIP-KNEE" removed due to being highly correlated with the target variable. "NumberOfReadmissions_HIP-KNEE" removed as this would be highly influenced by hospital size and we have no data on hospital sizes.
> Sepsis variables removed due to unclear definition in the dataset's dictionary of what the values represent.
> Score_PSI_90 variable removed because it's a summary of the other PSI variables. We chose to include all the individual PSI variables, which makes the summary variable redundant. 
> We chose to remove a lot of the patient survey data due to collinearity and redundancy. The average star rating data is redundant with the linear mean score data. We decided to keep the overall hospital rating linear mean score, and the hospital recommendation linear mean score columns. We felt that these variables summarized the other, more granular, metrics. For example COMP-1 is nurse responsiveness and COMP-2 is doctor responsiveness, COMP-3 is staff responsiveness. It makes sense that a lot of these were collinear. We had considered engineering the comp features (1-7) together into a single patient experience variable, however, this was collinear with overall hospital rating and recommendation. We also chose to go with the linear mean score overall hospital rating and recommendation score, because the dataset essentially already scaled these variables for us by performing a linear transformation. 
> It always seems a little scary removing entire chunks of variables, as we wouldn't want to miss any significant relationships between the variables. Do you think this is a wise decision? Are there any other ideas you could think of to engineer variables in a way to keep more of them? 

#### Reassess collinearity with heatmap and correlation matrix
```{r, fig.width=14, fig.height=15}
# Compute correlation matrix
cor_matrix <- cor(HipKneeTrain %>% select_if(is.numeric), use = "pairwise.complete.obs")

# Melt the correlation matrix into a long format
cor_melted <- melt(cor_matrix)

# Plot heatmap
ggplot(cor_melted, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Figure 5. Correlation Heatmap of Numeric Variables")

# Convert correlation matrix to df
cor_table <- as.data.frame(cor_matrix)

# Add variable names as a column
cor_table$Variable <- rownames(cor_table)

# Reorder columns
cor_table <- cor_table %>%
  select(Variable, everything())

# Print table
cor_table %>%
  kable(caption = "Table 8. Correlation Coefficients Table") %>%
  kable_styling(bootstrap_options = c("hover", "striped", "responsive"))
```

## Imputation and Handling of Missing Values (AC)

```{r}
# Change - to _ in HIP-KNEE
colnames(HipKneeTrain) <- gsub("-", "_", colnames(HipKneeTrain))

# Remove all NA values in target variable "PredictedReadmissionRate_HIP_KNEE"
HipKneeTrain <- HipKneeTrain %>% filter(!is.na(PredictedReadmissionRate_HIP_KNEE))

# Remove all NA values in the "State", "StateCode", and "FacilityName" columns
HipKneeTrain <- HipKneeTrain %>% drop_na(State, StateCode, FacilityName)


# Print number of remaining variables and observations
dimensions <- dim(HipKneeTrain)
cat("Number of variables:", dimensions[2], "\n")
cat("Number of observations:", dimensions[1], "\n")
```
> We decided to remove the one facility that had an NA value, which also happened to be the same observation with a missing state value.

```{r}
# Calculate missing values
missing_values_summary <- HipKneeTrain %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Missing_Count") %>%
  mutate(Missing_Percentage = (Missing_Count / nrow(HipKneeTrain)) * 100)

# Print table
missing_values_summary %>%
  kable(caption = "Table 7. Missing Values Summary") %>%
  kable_styling(bootstrap_options = c("hover", "striped", "responsive"))
```
#### Impute variables with low percentage missingness (<5%) by the median for numeric variables and mode for categorical variables
```{r}
# Calculate median for columns with <5% missing values
numeric_vars_low_missing <- c("HcahpsLinearMeanValue_H_HSP_RATING_LINEAR_SCORE", "EDV", "HCP_COVID_19", "IMM_3", "OP_18b", "SAFE_USE_OF_OPIOIDS", "Score_COMP_HIP_KNEE", "Score_PSI_03", "Score_PSI_06", "Score_PSI_08", "Score_PSI_09", "Score_PSI_10", "Score_PSI_11", "Score_PSI_12", "Score_PSI_13", "Score_PSI_14", "Score_PSI_15", "Payment_PAYM_90_HIP_KNEE")

for (var in numeric_vars_low_missing) {
  HipKneeTrain[[var]][is.na(HipKneeTrain[[var]])] <- median(HipKneeTrain[[var]], na.rm = TRUE)
}
```

#### Impute high percentage missingness variables (>5%) using KNN
```{r}
# Select high missingness variables for KNN imputation
vars_for_knn <- c("VTE_1", "Score_MORT_30_AMI", "Score_MORT_30_COPD", "Score_MORT_30_HF", "Score_MORT_30_PN", "Score_MORT_30_STK", "Score_PSI_04", "OP_29")

# Perform KNN imputation
HipKneeTrain_knn <- kNN(HipKneeTrain, variable = vars_for_knn, k = 5)

# Remove columns created by the KNN function
HipKneeTrain_knn <- HipKneeTrain_knn %>% select(-ends_with("_imp"))

# Update HipKneeTrain with imputed values
HipKneeTrain[vars_for_knn] <- HipKneeTrain_knn[vars_for_knn]
```

```{r}
# Calculate missing values
missing_values_summary <- HipKneeTrain %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Missing_Count") %>%
  mutate(Missing_Percentage = (Missing_Count / nrow(HipKneeTrain)) * 100)

# Print table
missing_values_summary %>%
  kable(caption = "Table 7. Missing Values Summary") %>%
  kable_styling(bootstrap_options = c("hover", "striped", "responsive"))
```

## Feature Engineer Mortality Data (AC)
```{r}
# Average death rates amongst mortality variables and create new column "Score_Ovr_MORT"
HipKneeTrain$Score_Ovr_MORT <- rowMeans(HipKneeTrain[, c("Score_MORT_30_AMI", 
                                                         "Score_MORT_30_COPD", 
                                                         "Score_MORT_30_HF", 
                                                         "Score_MORT_30_PN", 
                                                         "Score_MORT_30_STK")], 
                                                          na.rm = TRUE)

# Remove old mortality columns
HipKneeTrain <- HipKneeTrain[, !(names(HipKneeTrain) %in% c("Score_MORT_30_AMI", 
                                                            "Score_MORT_30_COPD",
                                                            "Score_MORT_30_HF", 
                                                            "Score_MORT_30_PN", 
                                                            "Score_MORT_30_STK"))]

```

#### Reassess heatmap with engineered mortality data
```{r, fig.width=14, fig.height=15}
# Compute correlation matrix
cor_matrix <- cor(HipKneeTrain %>% select_if(is.numeric), use = "pairwise.complete.obs")

# Melt the correlation matrix into a long format
cor_melted <- melt(cor_matrix)

# Plot heatmap
ggplot(cor_melted, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Figure 5. Correlation Heatmap of Numeric Variables")
```

#### Save the data for future ease of use
```{r}
save(HipKneeTrain, file = "HipKneeTrain.RData")
```

## Preprocessing the test dataset (AC)
We are utilizing the most recent snapshot from 04/24/2024 as our test set. Utilizing this brand new data will help to ensure that our model is generalizable and useful for future analyses. 

### Loading the Data 
```{r, message=FALSE, warning=FALSE}
# Set the directory for the data files
filepath <- "/Users/adelinecasali/Desktop/hospitals_04_2024/" 

# List the files in the directory that have "Hospital.csv"
files <- list.files(path = filepath, pattern = "Hospital.csv")

# Iterate through each file in the list
for(f in 1:length(files)) {
  
# Read the CSV, clean column names to upper camel case, and store in "dat"
    dat <- clean_names(read_csv(paste0(filepath, files[f]),
                                show_col_types = FALSE), 
                       case = "upper_camel")
    
# Remove ".Hospital.csv" part of the file names to create variable name
    filename <- gsub(".Hospital\\.csv", "", files[f])
    
# Assign data to a variable with the above created name
    assign(filename, dat)
}
# Create a df of file names without ".Hospital.csv"
files <- gsub(".Hospital\\.csv", "", files) %>% data.frame()

# Set column name of the df to "File Name"
names(files) <- "File Name"

files %>% 
  kable(
    format = "html",
    caption = "Table 1. List of hospital-level data files.") %>%
    kable_styling(bootstrap_options = c("striped", full_width = F)
  )
```

### Exploring and Preprocessing the FY_2024_Hospital_Readmissions_Reduction_Program dataset (AC)  

#### Viewing and checking for missing values  
```{r}
# Display first 10 rows of FY_2024_Hospital_Readmissions_Reduction_Program 
head(FY_2024_Hospital_Readmissions_Reduction_Program,10)

# Filter dataset to include numeric columns only
num_vars <- FY_2024_Hospital_Readmissions_Reduction_Program %>%
  select_if(is.numeric)

# Check for missing values
miss_vals <- sapply(num_vars, function(x) sum(is.na(x)))
print(miss_vals)
```

#### Replacing values with NA and "Too Few to Report" values with "5"  
```{r}
# Use the function "replace_with_na_all()" to replace aberrant values with NA
FY_2024_Hospital_Readmissions_Reduction_Program <- replace_with_na_all(FY_2024_Hospital_Readmissions_Reduction_Program, condition = ~ .x == "N/A")

# Replace "Too Few to Report" values with "5" in using gsub
FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions <- gsub("Too Few to Report", "5", FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions)

# Check first 10 rows to confirm that it worked
head(FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions, 10)

# NumberOfReadmissions had to be converted to numeric before applying integers
FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions <- as.numeric(FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions)

# Find all values of "5" in NumberOfReadmissions
fives <- which(FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions == 5)

# Replace values of "5" with random integers from 1 - 10
FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions[fives] <- sample(1:10, length(fives), replace = TRUE)

# Check the first 20 rows to see if this was applied correctly
head(FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions,20)
```

#### Converting columns to numeric  
```{r}
# Selecting the columns to convert
columns_to_convert <- c("NumberOfDischarges", "ExcessReadmissionRatio", "PredictedReadmissionRate", "ExpectedReadmissionRate", "NumberOfReadmissions")

# Use mutate_at to convert the specified columns to numeric
FY_2024_Hospital_Readmissions_Reduction_Program <- FY_2024_Hospital_Readmissions_Reduction_Program %>%
  mutate_at(vars(one_of(columns_to_convert)), as.numeric)

# Print the structure of the dataframe to check the changes
str(FY_2024_Hospital_Readmissions_Reduction_Program)
```

#### Removing excess text from measure names
```{r}
FY_2024_Hospital_Readmissions_Reduction_Program <-  FY_2024_Hospital_Readmissions_Reduction_Program %>%
  mutate(MeasureName = gsub("READM-30-", "", MeasureName)) %>% 
  mutate(MeasureName = gsub("-HRRP", "", MeasureName)) 
```

#### Pivoting the data wider  
```{r}
readmissionsClean <- FY_2024_Hospital_Readmissions_Reduction_Program %>%
  pivot_wider(
    names_from = MeasureName, 
    values_from = c(NumberOfDischarges, ExcessReadmissionRatio, PredictedReadmissionRate, ExpectedReadmissionRate, NumberOfReadmissions), 
    id_cols = c(FacilityName, FacilityId, State, StartDate, EndDate)
  )

# Check the new dataframe
dim(readmissionsClean)
head(readmissionsClean)
```

#### Filtering for only hip/knee conditions
```{r}
readmissionsClean <- readmissionsClean %>%
  select(FacilityName, FacilityId, State, matches("HIP-KNEE$"))
```

### Exploring and Preprocessing the HCAHPS dataset 

#### Viewing and checking for missing values  
```{r}
# Display first 10 rows of HCAHPS 
head(HCAHPS,10)

# Filter dataset to include numeric columns only
num_vars <- HCAHPS %>%
  select_if(is.numeric)

# Check for missing values
miss_vals <- sapply(num_vars, function(x) sum(is.na(x)))
print(miss_vals)
```

#### Removing footnote columns and replacing NA values  
```{r}
# Removing all footnote columns
HCAHPS <- HCAHPS %>%
  select(-ends_with("footnote"))

# Replacing all "Not Applicable" with NA
HCAHPS <- as.data.frame(sapply(HCAHPS, function(x) {
  if (is.character(x)) {
    x[x == "Not Applicable"] <- NA
  }
  return(x)
}))

# Replacing all "Not Available" with NA
HCAHPS <- as.data.frame(sapply(HCAHPS, function(x) {
  if (is.character(x)) {
    x[x == "Not Available"] <- NA
  }
  return(x)
}))
```

#### Pivoting the data wider  
```{r}
HCAHPSClean <- HCAHPS %>%
  pivot_wider(
    names_from = HcahpsMeasureId, 
    values_from = c(PatientSurveyStarRating, HcahpsAnswerPercent, HcahpsLinearMeanValue, SurveyResponseRatePercent), 
    id_cols = c(FacilityName, FacilityId, State)
  )

# Check the new dataframe
dim(HCAHPSClean)
head(HCAHPSClean)
```

### Exploring and Preprocessing the Timely_and_Effective_Care dataset 

#### Viewing and checking for missing values  
```{r}
# Display first 10 rows of Timely_and_Effective_Care
head(Timely_and_Effective_Care,10)

# Filter dataset to include numeric columns only
num_vars <- Timely_and_Effective_Care %>%
  select_if(is.numeric)

# Check for missing values
miss_vals <- sapply(num_vars, function(x) sum(is.na(x)))
print(miss_vals)
```

#### Replacing NA values  
```{r}
# Replacing all "Not Applicable" with NA
Timely_and_Effective_Care <- as.data.frame(sapply(Timely_and_Effective_Care, function(x) {
  if (is.character(x)) {
    x[x == "Not Applicable"] <- NA
  }
  return(x)
}))

# Replacing all "Not Available" with NA
Timely_and_Effective_Care <- as.data.frame(sapply(Timely_and_Effective_Care, function(x) {
  if (is.character(x)) {
    x[x == "Not Available"] <- NA
  }
  return(x)
}))
```

#### Pivoting the data wider  
```{r}
careClean <- Timely_and_Effective_Care %>%
  pivot_wider(
    names_from = MeasureId, 
    values_from = c(Score), 
    id_cols = c(FacilityName, FacilityId, State)
  )

# Check the new dataframe
dim(careClean)
head(careClean)
```

### Exploring and Preprocessing the Complications_and_Deaths dataset

#### Viewing and checking for missing values  
```{r}
# Display first 10 rows of Complications_and_Deaths
head(Complications_and_Deaths,10)

# Filter dataset to include numeric columns only
num_vars <- Complications_and_Deaths %>%
  select_if(is.numeric)

# Check for missing values
miss_vals <- sapply(num_vars, function(x) sum(is.na(x)))
print(miss_vals)
```

#### Replacing NA values  
```{r}
# Replacing all "Not Applicable" with NA
Complications_and_Deaths <- as.data.frame(sapply(Complications_and_Deaths, function(x) {
  if (is.character(x)) {
    x[x == "Not Applicable"] <- NA
  }
  return(x)
}))

# Replacing all "Not Available" with NA
Complications_and_Deaths <- as.data.frame(sapply(Complications_and_Deaths, function(x) {
  if (is.character(x)) {
    x[x == "Not Available"] <- NA
  }
  return(x)
}))
```

#### Pivoting the data wider  
```{r}
deathsClean <- Complications_and_Deaths %>%
  pivot_wider(
    names_from = MeasureId, 
    values_from = c(ComparedToNational, Score), 
    id_cols = c(FacilityName, FacilityId, State)
  )

# Check the new dataframe
dim(deathsClean)
head(deathsClean)
```

### Exploring and Preprocessing the Payment_and_Value_of_Care dataset 

#### Viewing and checking for missing values  
```{r}
# Display first 10 rows of Payment_and_Value_of_Care
head(Payment_and_Value_of_Care,10)

# Filter dataset to include numeric columns only
num_vars <- Payment_and_Value_of_Care %>%
  select_if(is.numeric)

# Check for missing values
miss_vals <- sapply(num_vars, function(x) sum(is.na(x)))
print(miss_vals)
```

#### Replacing NA values  
```{r}
# Replacing all "Not Applicable" with NA
Payment_and_Value_of_Care <- as.data.frame(sapply(Payment_and_Value_of_Care, function(x) {
  if (is.character(x)) {
    x[x == "Not Applicable"] <- NA
  }
  return(x)
}))

# Replacing all "Not Available" with NA
Payment_and_Value_of_Care <- as.data.frame(sapply(Payment_and_Value_of_Care, function(x) {
  if (is.character(x)) {
    x[x == "Not Available"] <- NA
  }
  return(x)
}))
```

#### Pivoting the data wider  
```{r}
paymentClean <- Payment_and_Value_of_Care %>%
  pivot_wider(
    names_from = PaymentMeasureId, 
    values_from = c(PaymentCategory, Payment), 
    id_cols = c(FacilityName, FacilityId, State)
  )

# Check the new dataframe
dim(paymentClean)
head(paymentClean)
```

#### Selecting only Hip-Knee related  
```{r}
paymentClean <- paymentClean %>%
  select(FacilityName, FacilityId, State, matches("HIP_KNEE$"))
```

### Joining and cleaning the datasets  

#### Joining the datasets based on FacilityId
```{r}
HipKneeCleanTest <- readmissionsClean %>%
  full_join(HCAHPSClean, by = "FacilityId") %>%
  full_join(careClean, by = "FacilityId") %>%
  full_join(deathsClean, by = "FacilityId") %>%
  full_join(paymentClean, by = "FacilityId")

head(HipKneeCleanTest)
```

#### Removing redundant columns  
```{r}
# Removing duplicate columns
HipKneeCleanTest <- HipKneeCleanTest %>%
  select(-matches("\\.(x|y|z|w|v)$"))
```

#### Checking for NA Values  
```{r, results='hide'}
# Checking the dimensions
dim(HipKneeCleanTest)

# Count NA values in each column
na_counts <- sapply(HipKneeCleanTest, function(x) sum(is.na(x)))

# View the NA counts
print(na_counts)
```

#### Removing columns with more than 80% NA values
```{r, results='hide'}
# Calculate the percentage of NA values for each column
na_percentage <- sapply(HipKneeCleanTest, function(x) mean(is.na(x)))

# Remove columns where more than 80% of the values are NA
HipKneeCleanTest <- HipKneeCleanTest[, na_percentage <= 0.8]

# Count NA values in each column
na_counts <- sapply(HipKneeCleanTest, function(x) sum(is.na(x)))

# View the NA counts
print(na_counts)

# Check the dimensions
dim(HipKneeCleanTest)
```

#### Removing answer percent and survey response percent columns
```{r}
# Remove columns containing 'AnswerPercent' or 'SurveyResponseRate'
HipKneeCleanTest <- HipKneeCleanTest %>%
  select(-matches("AnswerPercent|SurveyResponseRate"))

# Check the dimensions
dim(HipKneeCleanTest)
```

#### Removing compared to national columns  
```{r}
# Remove columns containing 'ComparedToNational' and 'PaymentCategory'
HipKneeCleanTest <- HipKneeCleanTest %>%
  select(-matches("ComparedToNational|PaymentCategory"))

# Check the dimensions
dim(HipKneeCleanTest)
```

#### Checking data structure
```{r}
str(HipKneeCleanTest)

# Convert columns to numeric
HipKneeCleanTest <- HipKneeCleanTest %>%
  mutate_at(vars(starts_with("PatientSurveyStarRating_"), 
                 starts_with("HcahpsLinearMeanValue_"), 
                 starts_with("Score_"),
                 starts_with("ED_"),
                 starts_with("IMM_"),
                 starts_with("OP_"),
                 starts_with("SEP_"),
                 starts_with("SEV_"),
                 starts_with("STK_"),
                 starts_with("VTE_"),
                 starts_with("SAFE_"),
                 starts_with("HCP_")),
            ~ as.numeric(as.character(.)))

# View the structure
str(HipKneeCleanTest)
```

#### Fixing the payment column
```{r}
# Remove $ and , and convert to numeric
HipKneeCleanTest <- HipKneeCleanTest %>%
  mutate_at(vars(starts_with("Payment_")), 
            ~ as.numeric(gsub("[\\$,]", "", .)))

# Checking the structure
str(HipKneeCleanTest)
```

### Encoding categorical variables

#### Identify Categorical Variables
```{r}
# Create function to find categorical variables
is_categorical <- function(x) is.factor(x) | is.character(x)

# Apply function to all variables in the dataset
categorical_vars <- sapply(HipKneeClean, is_categorical)

# Print the names of all categorical variables
categorical <- names(HipKneeClean)[categorical_vars]
categorical
```

#### Dummy encode the EDV column 
```{r}
# Define the encoding mapping (ignore NAs for now)
encoding_map <- c(
  'low' = 1,
  'medium' = 2,
  'high' = 3,
  'very high' = 4
)
# Dummy encoding used due to ordinal nature of this data

# Create a copy of HipKneeCleanTest and name it HipKneeTest to separate cleaned dataset and the test dataset
HipKneeTest <- HipKneeCleanTest %>%
  mutate(EDV = recode(EDV, !!!encoding_map))

# Print first 20 rows of EDV column in HipKneeClean and HipKneeTrain to ensure proper encoding
cat("HipKneeCleanTest")
print(head(HipKneeCleanTest$EDV, 20))

cat("HipKneeTest")
print(head(HipKneeTest$EDV, 20))
```

#### Encode each state in alphabetical order
```{r}
# Manually map out each state with their respective code in alphabetical order with a preceding 0 to make data non-ordinal
state_mapping <- c(
  "AL" = "001",
  "AK" = "002",
  "AZ" = "003",
  "AR" = "004",
  "CA" = "005",
  "CO" = "006",
  "CT" = "007",
  "DE" = "008",
  "FL" = "009",
  "GA" = "010",
  "HI" = "011",
  "ID" = "012",
  "IL" = "013",
  "IN" = "014",
  "IA" = "015",
  "KS" = "016",
  "KY" = "017",
  "LA" = "018",
  "ME" = "019",
  "MD" = "020",
  "MA" = "021",
  "MI" = "022",
  "MN" = "023",
  "MS" = "024",
  "MO" = "025",
  "MT" = "026",
  "NE" = "027",
  "NV" = "028",
  "NH" = "029",
  "NJ" = "030",
  "NM" = "031",
  "NY" = "032",
  "NC" = "033",
  "ND" = "034",
  "OH" = "035",
  "OK" = "036",
  "OR" = "037",
  "PA" = "038",
  "RI" = "039",
  "SC" = "040",
  "SD" = "041",
  "TN" = "042",
  "TX" = "043",
  "UT" = "044",
  "VT" = "045",
  "VA" = "046",
  "WA" = "047",
  "WV" = "048",
  "WI" = "049",
  "WY" = "050"
)

# Create new "StateCode" column with the encoded values
HipKneeTest <- HipKneeTest %>%
  mutate(StateCode = state_mapping[State])

# Print 100 rows of the "State" and "StateCode" columns to ensure accuracy
print("State and StateCode Columns")
print(head(HipKneeTest[c("State", "StateCode")], 100))

# Print all unique values in "StateCode" column to ensure accuracy
print("Unique StateCode Values")
print(unique(HipKneeTest$StateCode))
```

### Collinearity and Feature Removal

#### Remove correlated and unnecessary variables 
```{r}
# Specify columns to remove
columns_to_remove <- c(
  "ED_2_Strata_1",
  "OP_23",
  "VTE_2",
  "OP_18c",
  "OP_22",
  "STK_02",
  "STK_05",
  "STK_06",
  "HcahpsLinearMeanValue_H_RECMND_LINEAR_SCORE",
  "NumberOfReadmissions_HIP-KNEE",
  "ExcessReadmissionRatio_HIP-KNEE",
  "ExpectedReadmissionRate_HIP-KNEE",
  "SEP_1",
  "SEV_SEP_6HR",
  "SEV_SEP_3HR",
  "SEP_SH_6HR",
  "SEP_SH_3HR",
  "Score_PSI_90",
  "PatientSurveyStarRating_H_COMP_1_STAR_RATING",    
  "PatientSurveyStarRating_H_COMP_2_STAR_RATING",    
  "PatientSurveyStarRating_H_COMP_3_STAR_RATING",    
  "PatientSurveyStarRating_H_COMP_5_STAR_RATING",    
  "PatientSurveyStarRating_H_COMP_6_STAR_RATING",    
  "PatientSurveyStarRating_H_COMP_7_STAR_RATING",    
  "PatientSurveyStarRating_H_CLEAN_STAR_RATING",     
  "PatientSurveyStarRating_H_QUIET_STAR_RATING",     
  "PatientSurveyStarRating_H_HSP_RATING_STAR_RATING",
  "PatientSurveyStarRating_H_RECMND_STAR_RATING",    
  "PatientSurveyStarRating_H_STAR_RATING",
  "HcahpsLinearMeanValue_H_COMP_1_LINEAR_SCORE",    
  "HcahpsLinearMeanValue_H_COMP_2_LINEAR_SCORE",    
  "HcahpsLinearMeanValue_H_COMP_3_LINEAR_SCORE",    
  "HcahpsLinearMeanValue_H_COMP_5_LINEAR_SCORE",    
  "HcahpsLinearMeanValue_H_COMP_6_LINEAR_SCORE",    
  "HcahpsLinearMeanValue_H_COMP_7_LINEAR_SCORE",    
  "HcahpsLinearMeanValue_H_CLEAN_LINEAR_SCORE",     
  "HcahpsLinearMeanValue_H_QUIET_LINEAR_SCORE"
)

# Remove specified columns
HipKneeTest <- HipKneeTest %>% select(-all_of(columns_to_remove))
```

```{r}
# Print column names to verify
print("Remaining columns:")
print(colnames(HipKneeTest))
```

#### Reassess collinearity with heatmap and correlation matrix
```{r, fig.width=14, fig.height=15}
# Compute correlation matrix
cor_matrix <- cor(HipKneeTest %>% select_if(is.numeric), use = "pairwise.complete.obs")

# Melt the correlation matrix into a long format
cor_melted <- melt(cor_matrix)

# Plot heatmap
ggplot(cor_melted, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Figure 5. Correlation Heatmap of Numeric Variables")

# Convert correlation matrix to df
cor_table <- as.data.frame(cor_matrix)

# Add variable names as a column
cor_table$Variable <- rownames(cor_table)

# Reorder columns
cor_table <- cor_table %>%
  select(Variable, everything())

# Print table
cor_table %>%
  kable(caption = "Table 8. Correlation Coefficients Table") %>%
  kable_styling(bootstrap_options = c("hover", "striped", "responsive"))
```

### Imputation and Handling of Missing Values

```{r}
# Change - to _ in HIP-KNEE
colnames(HipKneeTest) <- gsub("-", "_", colnames(HipKneeTest))

# Remove all NA values in target variable "PredictedReadmissionRate_HIP_KNEE"
HipKneeTest <- HipKneeTest %>% filter(!is.na(PredictedReadmissionRate_HIP_KNEE))

# Remove all NA values in the "State", "StateCode", and "FacilityName" columns
HipKneeTest <- HipKneeTest %>% drop_na(State, StateCode, FacilityName)


# Print number of remaining variables and observations
dimensions <- dim(HipKneeTest)
cat("Number of variables:", dimensions[2], "\n")
cat("Number of observations:", dimensions[1], "\n")
```

```{r}
# Calculate missing values
missing_values_summary <- HipKneeTest %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Missing_Count") %>%
  mutate(Missing_Percentage = (Missing_Count / nrow(HipKneeTest)) * 100)

# Print table
missing_values_summary %>%
  kable(caption = "Table 7. Missing Values Summary") %>%
  kable_styling(bootstrap_options = c("hover", "striped", "responsive"))
```

#### Impute variables with low percentage missingness (<5%) by the median for numeric variables and mode for categorical variables
```{r}
# Calculate median for columns with <5% missing values
numeric_vars_low_missing <- c("HcahpsLinearMeanValue_H_HSP_RATING_LINEAR_SCORE", "EDV", "HCP_COVID_19", "IMM_3", "OP_18b", "SAFE_USE_OF_OPIOIDS", "Score_COMP_HIP_KNEE", "Score_PSI_03", "Score_PSI_06", "Score_PSI_08", "Score_PSI_09", "Score_PSI_10", "Score_PSI_11", "Score_PSI_12", "Score_PSI_13", "Score_PSI_14", "Score_PSI_15", "Payment_PAYM_90_HIP_KNEE")

for (var in numeric_vars_low_missing) {
  HipKneeTest[[var]][is.na(HipKneeTest[[var]])] <- median(HipKneeTest[[var]], na.rm = TRUE)
}
```

#### Impute high percentage missingness variables (>5%) using KNN
```{r}
# Select high missingness variables for KNN imputation
vars_for_knn <- c("VTE_1", "Score_MORT_30_AMI", "Score_MORT_30_COPD", "Score_MORT_30_HF", "Score_MORT_30_PN", "Score_MORT_30_STK", "Score_PSI_04", "OP_29")

# Perform KNN imputation
HipKneeTest_knn <- kNN(HipKneeTest, variable = vars_for_knn, k = 5)

# Remove columns created by the KNN function
HipKneeTest_knn <- HipKneeTest_knn %>% select(-ends_with("_imp"))

# Update HipKneeTrain with imputed values
HipKneeTest[vars_for_knn] <- HipKneeTest_knn[vars_for_knn]
```

```{r}
# Calculate missing values
missing_values_summary <- HipKneeTest %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Missing_Count") %>%
  mutate(Missing_Percentage = (Missing_Count / nrow(HipKneeTest)) * 100)

# Print table
missing_values_summary %>%
  kable(caption = "Table 7. Missing Values Summary") %>%
  kable_styling(bootstrap_options = c("hover", "striped", "responsive"))
```

#### Feature Engineer Mortality Data 
```{r}
# Average death rates amongst mortality variables and create new column "Score_Ovr_MORT"
HipKneeTest$Score_Ovr_MORT <- rowMeans(HipKneeTest[, c("Score_MORT_30_AMI", 
                                                         "Score_MORT_30_COPD", 
                                                         "Score_MORT_30_HF", 
                                                         "Score_MORT_30_PN", 
                                                         "Score_MORT_30_STK")], 
                                                          na.rm = TRUE)

# Remove old mortality columns
HipKneeTest <- HipKneeTest[, !(names(HipKneeTest) %in% c("Score_MORT_30_AMI", 
                                                            "Score_MORT_30_COPD",
                                                            "Score_MORT_30_HF", 
                                                            "Score_MORT_30_PN", 
                                                            "Score_MORT_30_STK"))]

```

#### Reassess heatmap with engineered mortality data
```{r, fig.width=14, fig.height=15}
# Compute correlation matrix
cor_matrix <- cor(HipKneeTest %>% select_if(is.numeric), use = "pairwise.complete.obs")

# Melt the correlation matrix into a long format
cor_melted <- melt(cor_matrix)

# Plot heatmap
ggplot(cor_melted, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Figure 5. Correlation Heatmap of Numeric Variables")
```

#### Save the data for future ease of use
```{r}
save(HipKneeTest, file = "HipKneeTest.RData")
```

## Descriptive Statistics
```{r}
# Create a summary table of descriptive statistics
descr_stats <- describe(HipKneeTrain)
# Remove the rows with Facility ID, State and State code, and facility name
descr_stats <- descr_stats %>% filter(vars != c(1, 23, 24, 26))
# Remove columns 1, 2, 5, and 6
descr_stats <- descr_stats[, -c(1, 2, 5, 6)]

# Create a table with kable
kable(descr_stats, format = "html", caption = "Descriptive Statistics for All Numeric Variables in Final Dataset") %>%
  kable_styling(
    bootstrap_options = c("hover", "striped", "responsive")
  ) %>%
  column_spec(1, bold = TRUE) %>%
  column_spec(2, width = "5em") %>%
  row_spec(0, bold = TRUE, background = "#f2f2f2")

# Select numeric columns
numeric_columns <- HipKneeTrain %>% select_if(is.numeric)

# Melt the data for easier plotting with ggplot2
numeric_melted <- melt(numeric_columns)

# Create histograms
ggplot(numeric_melted, aes(x = value)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  facet_wrap(~variable, scales = "free_x") +
  theme_minimal() +
  labs(title = "Histograms of Numeric Variables", x = "Value", y = "Frequency")
```

# Appendix 3: Segmentation Analysis Code


# Appendix 4: Initial Model Code


