---
title: 'Identifying Risk Factors and Preferred Hospitals for Hip/Knee Replacements:
  An Analysis of 2019-2022 Hospital Readmission Reduction Program Data'
author: "Adeline Casali and Scott Eugley"
date: "2024-07-09"
output:
  html_document:
    toc: true
    toc_float: true
    theme: flatly
    fig_crop: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = FALSE, 
                      cache.comments = TRUE,
                      size = 13)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# clean, set up, and load
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
rm(list = ls(all = TRUE))
set.seed(123)

pacman::p_load(tidyverse, 
               ggplot2, 
               kableExtra, 
               lubridate, 
               caret,
               janitor,
               naniar, 
               maps, 
               psych, 
               reshape2, 
               corrplot,
               VIM,
               glmnet,
               cluster,
               factoextra,
               dendextend,
               randomForest,
               caret, 
               lmtest,
               keras,
               e1071, 
               pROC, 
               ROCR) 
```

# Background and Question
Since 2012, the Centers for Medicare and Medicaid Services have implemented the Hospital Readmission Reduction Program (HRRP). This program tracks hospital readmission rates and incentivizes hospitals to reduce unnecessary readmissions through financial penalties. Using the 2019-2022 readmission data from the HRRP, this analysis aims to identify the preferred and non-preferred hospitals for hip and knee replacements for a health insurance company. Furthermore, it will examine the risk factors associated with higher readmission rates for these procedures.  

##### Question:  
What risk factors are associated with hospital readmission rates for hip/knee replacements?  

##### Motivation:  
Understanding these risk factors can help health insurance companies guide patients towards hospitals with better outcomes, thereby improving patient outcomes and reducing costs associated with readmissions.  

##### Need:  
The insights from this analysis can be used to improve hospital performance, enhance patient care, and reduce costs. As of 2019, the average cost of readmission after hip/knee surgery was $8,588, and avoiding that cost would be highly beneficial for health insurance companies and consumers alike (Phillips et al., 2019).  

##### Novelty:  
Previous analyses have used these same or similar datasets with Logistic Regression and Random Forest models to identify the most important risk factors as they pertain to hospital readmission rates for hip/knee replacements. We will be trying to improve on this type of analysis by improving the performance of the models using various techniques. Prior analyses have implemented Random Forest models to extract important risk factors, but no prior analyses have used Random Forest to classify hospitals as preferred or non-preferred for hip/knee replacement, based on the important risk factors.  

##### Hypothesis:  
Hospitals with better Hospital Consumer Assessment of Healthcare Providers and Systems (HCAHPS) scores will have lower readmission rates for hip/knee replacements because higher patient satisfaction often correlates with better overall care quality and patient outcomes, including reduced complications and better post-discharge support (Edwards et al., 2015).  

# Data and Analysis
We will be using the datasets from the Centers for Medicare and Medicaid Services (Centers for Medicare & Medicaid Services, 2024). Our target variable will be the readmission rate after hip/knee surgery, using data from 2019-2022. We will utilize predictors from the HCAHPS (Hospital Consumer Assessment of Healthcare Providers and Systems) dataset as well as Timely and Effective Care, containing information on average wait times and vaccination compliance, Complications and Deaths, containing information about the frequency of deaths and complications for procedures, and Payment and Spending metrics, which includes the costs associated with procedures. 

### Analysis Plan:
1.	Data Preprocessing:  
o	Handle missing values, outliers, and inconsistencies in the dataset.  
o	Exploratory data analysis will be performed.  
2.	Model Selection:  
o	Random Forest: This model will be used to determine the most significant risk factors associated with hospital readmission, post hip/knee replacement surgery. The model will then be used in a binary classification task, which will categorize hospitals as either “preferred” or “non-preferred”, based on the national average for hospital readmission post hip/knee surgery.  
o	Elastic Net: While prior analyses have used coefficients from Logistic Regression modeling to determine significant risk factors, this analysis will leverage L1 and L2 regularization via an Elastic Net model.   
o	Neural Network: After using Elastic Net and Random Forest to find significant risk factors, these risk factors will be utilized in a neural network model and used to classify hospitals as either “preferred” or “non-preferred”. Classification ability of the neural network model will then be compared with the classification ability of the Random Forest model.   
3.	Feature Importance Analysis:  
o	Identify which predictors significantly influence hospital readmission rate, post hip/knee replacement surgery.  
o	Linear regression analysis (Elastic Net) significant risk factors will be compared to tree-based analysis (Random Forest) significant risk factors for overlap.  
4.	Model Validation:  
o	Validate the model using cross-validation techniques (k-fold, nested) to ensure robustness and generalizability.  
o	Hyperparameter tuning will be performed. Assessment of the model’s performance will be based on accuracy, AUC, and ROC metrics.  
o	The 2024 data will be used as the test set for this analysis.  

### Assessment:
##### Successful Analysis:  
We will consider our analysis successful if we can identify clear risk factors associated with hospital readmission rates and accurately classify hospitals as preferred or non-preferred.  

##### Hypothesis Support:  
Our hypothesis will be supported if hospitals with better HCAHPS scores demonstrate statistically significantly lower readmission rates for hip/knee replacements.  

##### Pitfalls:  
A potential pitfall of our analysis plan is data quality and completeness. The dataset does contain missing values, and it will need to be preprocessed to handle these missing values, outliers, and inconsistencies. Another potential pitfall is not having adequate computing power to implement deep learning with the size of our dataset. Lastly, a pitfall that we need to keep an eye out for is overfitting. We will know we have overfitting if the train set far outperforms the test set, in terms of model accuracy. 

# Exploratory Data Analysis  

## Data loading and preprocessing  

### Loading the Data (AC)  
```{r, message=FALSE, warning=FALSE}
# Set the directory for the data files
filepath <- "/Users/adelinecasali/Desktop/hospitals_current_data/" 

# List the files in the directory that have "Hospital.csv"
files <- list.files(path = filepath, pattern = "Hospital.csv")

# Iterate through each file in the list
for(f in 1:length(files)) {
  
# Read the CSV, clean column names to upper camel case, and store in "dat"
    dat <- clean_names(read_csv(paste0(filepath, files[f]),
                                show_col_types = FALSE), 
                       case = "upper_camel")
    
# Remove ".Hospital.csv" part of the file names to create variable name
    filename <- gsub(".Hospital\\.csv", "", files[f])
    
# Assign data to a variable with the above created name
    assign(filename, dat)
}
# Create a df of file names without ".Hospital.csv"
files <- gsub(".Hospital\\.csv", "", files) %>% data.frame()

# Set column name of the df to "File Name"
names(files) <- "File Name"

files %>% 
  kable(
    format = "html",
    caption = "Table 1. List of hospital-level data files.") %>%
    kable_styling(bootstrap_options = c("striped", full_width = F)
  )
```

### Exploring and Preprocessing the FY_2024_Hospital_Readmissions_Reduction_Program dataset (AC)  

#### Viewing and checking for missing values  
```{r}
# Display first 10 rows of FY_2024_Hospital_Readmissions_Reduction_Program 
head(FY_2024_Hospital_Readmissions_Reduction_Program,10)

# Filter dataset to include numeric columns only
num_vars <- FY_2024_Hospital_Readmissions_Reduction_Program %>%
  select_if(is.numeric)

# Check for missing values
miss_vals <- sapply(num_vars, function(x) sum(is.na(x)))
print(miss_vals)
```

#### Replacing values with NA and "Too Few to Report" values with "5"  
```{r}
# Use the function "replace_with_na_all()" to replace aberrant values with NA
FY_2024_Hospital_Readmissions_Reduction_Program <- replace_with_na_all(FY_2024_Hospital_Readmissions_Reduction_Program, condition = ~ .x == "N/A")

# Replace "Too Few to Report" values with "5" in using gsub
FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions <- gsub("Too Few to Report", "5", FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions)

# Check first 10 rows to confirm that it worked
head(FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions, 10)

# NumberOfReadmissions had to be converted to numeric before applying integers
FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions <- as.numeric(FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions)

# Find all values of "5" in NumberOfReadmissions
fives <- which(FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions == 5)

# Replace values of "5" with random integers from 1 - 10
FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions[fives] <- sample(1:10, length(fives), replace = TRUE)

# Check the first 20 rows to see if this was applied correctly
head(FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions,20)
```

#### Converting columns to numeric  
```{r}
# Selecting the columns to convert
columns_to_convert <- c("NumberOfDischarges", "ExcessReadmissionRatio", "PredictedReadmissionRate", "ExpectedReadmissionRate", "NumberOfReadmissions")

# Use mutate_at to convert the specified columns to numeric
FY_2024_Hospital_Readmissions_Reduction_Program <- FY_2024_Hospital_Readmissions_Reduction_Program %>%
  mutate_at(vars(one_of(columns_to_convert)), as.numeric)

# Print the structure of the dataframe to check the changes
str(FY_2024_Hospital_Readmissions_Reduction_Program)
```

#### Removing excess text from measure names
```{r}
FY_2024_Hospital_Readmissions_Reduction_Program <-  FY_2024_Hospital_Readmissions_Reduction_Program %>%
  mutate(MeasureName = gsub("READM-30-", "", MeasureName)) %>% 
  mutate(MeasureName = gsub("-HRRP", "", MeasureName)) 
```

#### Creating a dictionary for medical conditions  
```{r}
dict <- tribble(
  ~Acronym, ~Definition,
  "HIP-KNEE", "Total Hip/Knee Arthroplasty",
  "HF", "Heart Failure",
  "COPD", "Chronic Obstructive Pulmonary Disease",
  "AMI", "Acute Myocardial Infarction",
  "CABG", "Coronary Artery Bypass Graft",
  "PN", "Pneumonia"
)
dict %>% 
  kable(
    format = "html",
    caption = "Table 2. Acronyms of medical conditions for which hospital readmissions are tracked.") %>%
    kable_styling(bootstrap_options = c("hover", full_width = F)
  )
```

#### Pivoting the data wider  
```{r}
readmissionsClean <- FY_2024_Hospital_Readmissions_Reduction_Program %>%
  pivot_wider(
    names_from = MeasureName, 
    values_from = c(NumberOfDischarges, ExcessReadmissionRatio, PredictedReadmissionRate, ExpectedReadmissionRate, NumberOfReadmissions), 
    id_cols = c(FacilityName, FacilityId, State, StartDate, EndDate)
  )

# Check the new dataframe
dim(readmissionsClean)
head(readmissionsClean)
```

#### Filtering for only hip/knee conditions
```{r}
readmissionsClean <- readmissionsClean %>%
  select(FacilityName, FacilityId, State, matches("HIP-KNEE$"))
```

### Exploring and Preprocessing the HCAHPS dataset (AC)  

#### Viewing and checking for missing values  
```{r}
# Display first 10 rows of HCAHPS 
head(HCAHPS,10)

# Filter dataset to include numeric columns only
num_vars <- HCAHPS %>%
  select_if(is.numeric)

# Check for missing values
miss_vals <- sapply(num_vars, function(x) sum(is.na(x)))
print(miss_vals)
```

#### Removing footnote columns and replacing NA values  
```{r}
# Removing all footnote columns
HCAHPS <- HCAHPS %>%
  select(-ends_with("footnote"))

# Replacing all "Not Applicable" with NA
HCAHPS <- as.data.frame(sapply(HCAHPS, function(x) {
  if (is.character(x)) {
    x[x == "Not Applicable"] <- NA
  }
  return(x)
}))

# Replacing all "Not Available" with NA
HCAHPS <- as.data.frame(sapply(HCAHPS, function(x) {
  if (is.character(x)) {
    x[x == "Not Available"] <- NA
  }
  return(x)
}))
```

#### Creating a dictionary for HCAHPS questions
```{r}
dictHCAHPS <- tribble(
  ~`Measure ID`, ~`Measure Name`,
  "H-CLEAN-HSP-A-P", "Patients who reported that their room and bathroom were 'Always' clean",
  "H-CLEAN-HSP-SN-P", "Patients who reported that their room and bathroom were 'Sometimes' or 'Never' clean",
  "H-CLEAN-HSP-U-P", "Patients who reported that their room and bathroom were 'Usually' clean",
  "H-CLEAN-HSP-STAR-RATING", "Cleanliness - star rating",
  "H_CLEAN_LINEAR_SCORE", "Cleanliness - linear mean score",
  "H-COMP-1-A-P", "Patients who reported that their nurses 'Always' communicated well",
  "H-COMP-1-SN-P", "Patients who reported that their nurses 'Sometimes' or 'Never' communicated well",
  "H-COMP-1-U-P", "Patients who reported that their nurses 'Usually' communicated well",
  "H-COMP-1-STAR-RATING", "Nurse communication - star rating",
  "H_COMP_1_LINEAR_SCORE", "Nurse communication - linear mean score",
  "H-COMP-2-A-P", "Patients who reported that their doctors 'Always' communicated well",
  "H-COMP-2-SN-P", "Patients who reported that their doctors 'Sometimes' or 'Never' communicated well",
  "H-COMP-2-U-P", "Patients who reported that their doctors 'Usually' communicated well",
  "H-COMP-2-STAR-RATING", "Doctor communication - star rating",
  "H_COMP_2_LINEAR_SCORE", "Doctor communication - linear mean score",
  "H-COMP-3-A-P", "Patients who reported that they 'Always' received help as soon as they wanted",
  "H-COMP-3-SN-P", "Patients who reported that they 'Sometimes' or 'Never' received help as soon as they wanted",
  "H-COMP-3-U-P", "Patients who reported that they 'Usually' received help as soon as they wanted",
  "H-COMP-3-STAR-RATING", "Staff responsiveness - star rating",
  "H_COMP_3_LINEAR_SCORE", "Staff responsiveness - linear mean score",
  "H-COMP-5-A-P", "Patients who reported that staff 'Always' explained about medicines before giving it to them",
  "H-COMP-5-SN-P", "Patients who reported that staff 'Sometimes' or 'Never' explained about medicines before giving it to them",
  "H-COMP-5-U-P", "Patients who reported that staff 'Usually' explained about medicines before giving it to them",
  "H-COMP-5-STAR-RATING", "Communication about medicine - star rating",
  "H_COMP_5_LINEAR_SCORE", "Communication about medicines - linear mean score",
  "H-COMP-6-N-P", "Patients who reported that NO, they were not given information about what to do during their recovery at home",
  "H-COMP-6-Y-P", "Patients who reported that YES, they were given information about what to do during their recovery at home",
  "H-COMP-6-STAR-RATING", "Discharge information - star rating",
  "H_COMP_6_LINEAR_SCORE", "Discharge information - linear mean score",
  "H-COMP-7-A", "Patients who 'Agree' they understood their care when they left the hospital",
  "H-COMP-7-D-SD", "Patients who 'Disagree' or 'Strongly Disagree' that they understood their care when they left the hospital",
  "H-COMP-7-SA", "Patients who 'Strongly Agree' that they understood their care when they left the hospital",
  "H-COMP-7-STAR-RATING", "Care transition - star rating",
  "H_COMP_7_LINEAR_SCORE", "Care transition - linear mean score",
  "H-HSP-RATING-0-6", "Patients who gave their hospital a rating of 6 or lower on a scale from 0 (lowest) to 10 (highest)",
  "H-HSP-RATING-7-8", "Patients who gave their hospital a rating of 7 or 8 on a scale from 0 (lowest) to 10 (highest)",
  "H-HSP-RATING-9-10", "Patients who gave their hospital a rating of 9 or 10 on a scale from 0 (lowest) to 10 (highest)",
  "H-HSP-RATING-STAR-RATING", "Overall rating of hospital - star rating",
  "H_HSP_RATING_LINEAR_SCORE", "Overall hospital rating - linear mean score",
  "H-QUIET-HSP-A-P", "Patients who reported that the area around their room was 'Always' quiet at night",
  "H-QUIET-HSP-SN-P", "Patients who reported that the area around their room was 'Sometimes' or 'Never' quiet at night",
  "H-QUIET-HSP-U-P", "Patients who reported that the area around their room was 'Usually' quiet at night",
  "H-QUIET-HSP-STAR-RATING", "Quietness - star rating",
  "H_QUIET_LINEAR_SCORE", "Quietness - linear mean score",
  "H-RECMND-DN", "Patients who reported NO, they would probably not or definitely not recommend the hospital",
  "H-RECMND-DY", "Patients who reported YES, they would definitely recommend the hospital",
  "H-RECMND-PY", "Patients who reported YES, they would probably recommend the hospital",
  "H-RECMND-STAR-RATING", "Recommend hospital - star rating",
  "H_RECMND_LINEAR_SCORE", "Recommend hospital - linear mean score",
  "H-STAR-RATING", "Summary star rating"
)

dictHCAHPS %>% 
  kable(
    format = "html",
    caption = "Table 3. Measure IDs and Measure Names from HCAHPS") %>%
    kable_styling(bootstrap_options = c("hover", "full_width" = F))
```


#### Pivoting the data wider  
```{r}
HCAHPSClean <- HCAHPS %>%
  pivot_wider(
    names_from = HcahpsMeasureId, 
    values_from = c(PatientSurveyStarRating, HcahpsAnswerPercent, HcahpsLinearMeanValue, SurveyResponseRatePercent), 
    id_cols = c(FacilityName, FacilityId, State)
  )

# Check the new dataframe
dim(HCAHPSClean)
head(HCAHPSClean)
```

### Exploring and Preprocessing the Timely_and_Effective_Care dataset (AC)  

#### Viewing and checking for missing values  
```{r}
# Display first 10 rows of Timely_and_Effective_Care
head(Timely_and_Effective_Care,10)

# Filter dataset to include numeric columns only
num_vars <- Timely_and_Effective_Care %>%
  select_if(is.numeric)

# Check for missing values
miss_vals <- sapply(num_vars, function(x) sum(is.na(x)))
print(miss_vals)
```

#### Replacing NA values  
```{r}
# Replacing all "Not Applicable" with NA
Timely_and_Effective_Care <- as.data.frame(sapply(Timely_and_Effective_Care, function(x) {
  if (is.character(x)) {
    x[x == "Not Applicable"] <- NA
  }
  return(x)
}))

# Replacing all "Not Available" with NA
Timely_and_Effective_Care <- as.data.frame(sapply(Timely_and_Effective_Care, function(x) {
  if (is.character(x)) {
    x[x == "Not Available"] <- NA
  }
  return(x)
}))
```

#### Creating a dictionary Timely and Effective Care measure names
```{r}
dictCare <- tribble(
  ~`Measure ID`, ~`Measure Name`,
  "EDV", "Emergency department volume (alternate Measure ID: EDV-1)",
  "ED-2", "Average (median) admit decision time to time of departure from the emergency department for emergency department patients admitted to inpatient status",
  "IMM-3", "Healthcare workers given influenza vaccination",
  "HCP COVID-19", "COVID-19 Vaccination Coverage Among HCP",
  "OP-18b", "Average (median) time patients spent in the emergency department before leaving from the visit (alternate Measure ID: OP-18)",
  "OP-18c", "Average time patients spent in the emergency department before being sent home (Median Time from ED Arrival to ED Departure for Discharged ED Patients – Psychiatric/Mental Health Patients) *This measure is only found in the downloadable database, it is not displayed on Hospital Care Compare",
  "OP-22", "Percentage of patients who left the emergency department before being seen",
  "OP-23", "Percentage of patients who came to the emergency department with stroke symptoms who received brain scan results within 45 minutes of arrival",
  "OP-29", "Percentage of patients receiving appropriate recommendation for follow-up screening colonoscopy",
  "OP-31", "Percentage of patients who had cataract surgery and had improvement in visual function within 90 days following the surgery",
  "SEP-1", "Severe Sepsis and Septic Shock",
  "SEP-SH-3HR", "Septic Shock 3 Hour",
  "SEP-SH-6HR", "Septic Shock 6 Hour",
  "SEV-SEP-3HR", "Severe Sepsis 3 Hour",
  "SEV-SEP-6HR", "Severe Sepsis 6 Hour",
  "STK-02", "Percentage of ischemic stroke patients prescribed or continuing to take antithrombotic therapy at hospital discharge",
  "STK-03", "Percentage of ischemic stroke patients with atrial fibrillation/flutter who are prescribed or continuing to take anticoagulation therapy at hospital discharge",
  "STK-05", "Percentage of ischemic stroke patients administered antithrombotic therapy by the end of hospital day 2",
  "STK-06", "Percentage of ischemic stroke patients who are prescribed or continuing to take statin medication at hospital discharge",
  "VTE-1", "Percentage of patients that received VTE prophylaxis after hospital admission or surgery",
  "VTE-2", "Percentage of patients that received VTE prophylaxis after being admitted to the intensive care unit (ICU)",
  "Safe Use of Opioids", "Percentage of patients who were prescribed 2 or more opioids or an opioid and benzodiazepine concurrently at discharge"
)

dictCare %>% 
  kable(
    format = "html",
    caption = "Table 4. Measure IDs and Measure Names from Timely and Effective Care") %>%
    kable_styling(bootstrap_options = c("hover", "full_width" = F))
```

#### Pivoting the data wider  
```{r}
careClean <- Timely_and_Effective_Care %>%
  pivot_wider(
    names_from = MeasureId, 
    values_from = c(Score), 
    id_cols = c(FacilityName, FacilityId, State)
  )

# Check the new dataframe
dim(careClean)
head(careClean)
```

### Exploring and Preprocessing the Complications_and_Deaths dataset (AC)  

#### Viewing and checking for missing values  
```{r}
# Display first 10 rows of Complications_and_Deaths
head(Complications_and_Deaths,10)

# Filter dataset to include numeric columns only
num_vars <- Complications_and_Deaths %>%
  select_if(is.numeric)

# Check for missing values
miss_vals <- sapply(num_vars, function(x) sum(is.na(x)))
print(miss_vals)
```

#### Replacing NA values  
```{r}
# Replacing all "Not Applicable" with NA
Complications_and_Deaths <- as.data.frame(sapply(Complications_and_Deaths, function(x) {
  if (is.character(x)) {
    x[x == "Not Applicable"] <- NA
  }
  return(x)
}))

# Replacing all "Not Available" with NA
Complications_and_Deaths <- as.data.frame(sapply(Complications_and_Deaths, function(x) {
  if (is.character(x)) {
    x[x == "Not Available"] <- NA
  }
  return(x)
}))
```

#### Creating a dictionary for Complications and Deaths measure names
```{r}
dictDeaths <- tribble(
  ~`Measure ID`, ~`Measure Name`,
  "COMP-HIP-KNEE", "Rate of complications for hip/knee replacement patients",
  "PSI 90", "Serious complications (this is a composite or summary measure; alternate Measure ID: PSI-90-SAFETY)",
  "PSI 03", "Pressure sores (alternate Measure ID: PSI_3_Ulcer)",
  "PSI 04", "Deaths among patients with serious treatable complications after surgery (alternate Measure ID: PSI-4-SURG-COMP)",
  "PSI 06", "Collapsed lung due to medical treatment (alternate Measure ID: PSI-6-IAT-PTX)",
  "PSI 08", "Broken hip from a fall after surgery (alternate Measure ID: PSI_8_POST_HIP)",
  "PSI 09", "Postoperative hemorrhage or hematoma rate (alternate Measure ID: PSI_9_POST_HEM)",
  "PSI 10", "Kidney and diabetic complications after surgery (alternate Measure ID: PSI_10_POST_KIDNEY)",
  "PSI 11", "Respiratory failure after surgery (alternate Measure ID: PSI_11_POST_RESP)",
  "PSI 12", "Serious blood clots after surgery (alternate Measure ID: PSI-12-POSTOP-PULMEMB-DVT)",
  "PSI 13", "Blood stream infection after surgery (alternate Measure ID: PSI_13_POST_SEPSIS)",
  "PSI 14", "A wound that splits open after surgery on the abdomen or pelvis (alternate Measure ID: PSI-14-POSTOP-DEHIS)",
  "PSI 15", "Accidental cuts and tears from medical treatment (alternate Measure ID: PSI-15-ACC-LAC)",
  "MORT-30-AMI", "Death rate for heart attack patients",
  "MORT-30-CABG", "Death rate for Coronary Artery Bypass Graft (CABG) surgery patients",
  "MORT-30-COPD", "Death rate for chronic obstructive pulmonary disease (COPD) patients",
  "MORT-30-HF", "Death rate for heart failure patients",
  "MORT-30-PN", "Death rate for pneumonia patients",
  "MORT-30-STK", "Death rate for stroke patients"
)

dictDeaths %>% 
  kable(
    format = "html",
    caption = "Table 5. Measure IDs and Measure Names from Complications and Deaths") %>%
    kable_styling(bootstrap_options = c("hover", "full_width" = F))
```

#### Pivoting the data wider  
```{r}
deathsClean <- Complications_and_Deaths %>%
  pivot_wider(
    names_from = MeasureId, 
    values_from = c(ComparedToNational, Score), 
    id_cols = c(FacilityName, FacilityId, State)
  )

# Check the new dataframe
dim(deathsClean)
head(deathsClean)
```

### Exploring and Preprocessing the Payment_and_Value_of_Care dataset (AC)  

#### Viewing and checking for missing values  
```{r}
# Display first 10 rows of Payment_and_Value_of_Care
head(Payment_and_Value_of_Care,10)

# Filter dataset to include numeric columns only
num_vars <- Payment_and_Value_of_Care %>%
  select_if(is.numeric)

# Check for missing values
miss_vals <- sapply(num_vars, function(x) sum(is.na(x)))
print(miss_vals)
```

#### Replacing NA values  
```{r}
# Replacing all "Not Applicable" with NA
Payment_and_Value_of_Care <- as.data.frame(sapply(Payment_and_Value_of_Care, function(x) {
  if (is.character(x)) {
    x[x == "Not Applicable"] <- NA
  }
  return(x)
}))

# Replacing all "Not Available" with NA
Payment_and_Value_of_Care <- as.data.frame(sapply(Payment_and_Value_of_Care, function(x) {
  if (is.character(x)) {
    x[x == "Not Available"] <- NA
  }
  return(x)
}))
```

#### Creating a dictionary for Payment and Value of Care measure names
```{r}
dictPayment <- tribble(
  ~`Measure ID`, ~`Measure Name`,
  "PAYM-30-AMI", "Payment for heart attack patients",
  "PAYM-30-HF", "Payment for heart failure patients",
  "PAYM-30-PN", "Payment for pneumonia patients",
  "PAYM_90_HIP_KNEE", "Payment for hip/knee replacement patients"
)

dictPayment %>% 
  kable(
    format = "html",
    caption = "Table 6. Measure IDs and Measure Names from Payment and Value of Care") %>%
    kable_styling(bootstrap_options = c("hover", "full_width" = F))
```

#### Pivoting the data wider  
```{r}
paymentClean <- Payment_and_Value_of_Care %>%
  pivot_wider(
    names_from = PaymentMeasureId, 
    values_from = c(PaymentCategory, Payment), 
    id_cols = c(FacilityName, FacilityId, State)
  )

# Check the new dataframe
dim(paymentClean)
head(paymentClean)
```

#### Selecting only Hip-Knee related  
```{r}
paymentClean <- paymentClean %>%
  select(FacilityName, FacilityId, State, matches("HIP_KNEE$"))
```

### Joining and cleaning the datasets (AC)  

#### Joining the datasets based on FacilityId
```{r}
HipKneeClean <- readmissionsClean %>%
  full_join(HCAHPSClean, by = "FacilityId") %>%
  full_join(careClean, by = "FacilityId") %>%
  full_join(deathsClean, by = "FacilityId") %>%
  full_join(paymentClean, by = "FacilityId")

head(HipKneeClean)
```

#### Removing redundant columns  
```{r}
# Removing duplicate columns
HipKneeClean <- HipKneeClean %>%
  select(-matches("\\.(x|y|z|w|v)$"))
```

#### Checking for NA Values  
```{r, results='hide'}
# Checking the dimensions
dim(HipKneeClean)

# Count NA values in each column
na_counts <- sapply(HipKneeClean, function(x) sum(is.na(x)))

# View the NA counts
print(na_counts)
```

#### Removing columns with more than 80% NA values
```{r, results='hide'}
# Calculate the percentage of NA values for each column
na_percentage <- sapply(HipKneeClean, function(x) mean(is.na(x)))

# Remove columns where more than 80% of the values are NA
HipKneeClean <- HipKneeClean[, na_percentage <= 0.8]

# Count NA values in each column
na_counts <- sapply(HipKneeClean, function(x) sum(is.na(x)))

# View the NA counts
print(na_counts)

# Check the dimensions
dim(HipKneeClean)
```

#### Removing answer percent and survey response percent columns
```{r}
# Remove columns containing 'AnswerPercent' or 'SurveyResponseRate'
HipKneeClean <- HipKneeClean %>%
  select(-matches("AnswerPercent|SurveyResponseRate"))

# Check the dimensions
dim(HipKneeClean)
```

#### Removing compared to national columns
```{r}
# Remove columns containing 'ComparedToNational' and 'PaymentCategory'
HipKneeClean <- HipKneeClean %>%
  select(-matches("ComparedToNational|PaymentCategory"))

# Check the dimensions
dim(HipKneeClean)
```

#### Checking data structure
```{r}
str(HipKneeClean)

# Convert columns to numeric
HipKneeClean <- HipKneeClean %>%
  mutate_at(vars(starts_with("PatientSurveyStarRating_"), 
                 starts_with("HcahpsLinearMeanValue_"), 
                 starts_with("Score_"),
                 starts_with("ED_"),
                 starts_with("IMM_"),
                 starts_with("OP_"),
                 starts_with("SEP_"),
                 starts_with("SEV_"),
                 starts_with("STK_"),
                 starts_with("VTE_"),
                 starts_with("SAFE_"),
                 starts_with("HCP_")),
            ~ as.numeric(as.character(.)))

# View the structure
str(HipKneeClean)
```

#### Fixing the payment column
```{r}
# Remove $ and , and convert to numeric
HipKneeClean <- HipKneeClean %>%
  mutate_at(vars(starts_with("Payment_")), 
            ~ as.numeric(gsub("[\\$,]", "", .)))

# Checking the structure
str(HipKneeClean)
```

#### Saving the data to use without having to clean it
```{r}
save(HipKneeClean, file = "HipKneeClean.RData")
```


## Exploring the cleaned data  

### Generating visualizations (SE)  

#### Creating a summary table for numeric variables
```{r}
# Select numeric columns
numeric_columns <- select_if(HipKneeClean, is.numeric)

# Calculate descriptive statistics
descr_stats <- psych::describe(numeric_columns)

# Convert to a data frame
descr_stats_df <- as.data.frame(descr_stats)

# Display the table
kable(descr_stats_df, format = "html", caption = "Table 6. Descriptive Statistics for Numeric Variables in Cleaned Dataset") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

#### Exploring categorical variables
```{r}
# Visualizing the distribution of EDV (Emergency Department Volume)
ggplot(HipKneeClean, aes(x = EDV)) +
  geom_bar(fill = "skyblue", color = "black", alpha = 0.7) +
  labs(title = "Figure 1. Distribution of Emergency Department Volume",
       x = "EDV",
       y = "Count") +
  theme_minimal() + 
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank())
```

#### Visualizing the number of facilities per state
```{r}
# Data preparation
facility_counts <- HipKneeClean %>%
  group_by(State) %>%
  summarise(Count = n(), .groups = 'drop')

# Check the first few rows
head(facility_counts)

# Get state boundaries
states_map <- map_data("state")

# Create a mapping from state abbreviations to full state names
state_mapping <- data.frame(
  State = state.abb,
  full_state_name = tolower(state.name),
  stringsAsFactors = FALSE
)

# Add full state names to facility_counts
facility_counts <- merge(facility_counts, state_mapping, by.x = "State", by.y = "State")

# Join facility counts with state map data
facility_map_data <- left_join(states_map, facility_counts, by = c("region" = "full_state_name"))

# Replace NA values with 0 in the Count column
facility_map_data$Count[is.na(facility_map_data$Count)] <- 0

# Plot the map with facility counts
ggplot(data = facility_map_data) +
  geom_polygon(aes(x = long, y = lat, group = group, fill = Count), color = "white") +
  scale_fill_gradient(low = "lightblue", high = "darkblue", na.value = "grey50", name = "Facility Count") +
  theme_minimal() +
  labs(title = "Figure 2. Number of Facilities per State") +
  theme(axis.text = element_blank(),
        axis.title = element_blank(),
        panel.grid = element_blank(),
        plot.background = element_blank())
```

#### Visualizing the average PredictedReadmissionRate_HIP-KNEE per state
```{r}
# Rename column
HipKneeClean <- HipKneeClean %>%
  rename(PredictedReadmissionRate_HIP_KNEE = `PredictedReadmissionRate_HIP-KNEE`)

# Calculate the average PredictedReadmissionRate_HIP-KNEE per state
average_readmission_rate <- HipKneeClean %>%
  group_by(State) %>%
  summarize(Average_PredictedReadmissionRate_HIP_KNEE = mean(PredictedReadmissionRate_HIP_KNEE, na.rm = TRUE))

# Add full state names to the average readmission rate data
average_readmission_rate <- merge(average_readmission_rate, state_mapping, by.x = "State", by.y = "State")

# Join average readmission rate with state map data
readmission_map_data <- left_join(states_map, average_readmission_rate, by = c("region" = "full_state_name"))

# Plot the map with average readmission rates
ggplot(data = readmission_map_data) +
  geom_polygon(aes(x = long, y = lat, group = group, fill = Average_PredictedReadmissionRate_HIP_KNEE), color = "white") +
  scale_fill_gradient(low = "lightgreen", high = "darkgreen", name = "Average Predicted\nReadmission Rate") +
  theme_minimal() +
  labs(title = "Figure 3. Average Predicted Readmission Rate for Hip/Knee Replacement per State") +
  theme(axis.text = element_blank(),
        axis.title = element_blank(),
        panel.grid = element_blank(),
        plot.background = element_blank())
```

#### Visualizing the spread of the target variable (PredictedReadmissionRate_HIP_KNEE)
```{r}
# Create a histogram of PredictedReadmissionRate_HIP_KNEE
ggplot(HipKneeClean, aes(x = PredictedReadmissionRate_HIP_KNEE)) +
  geom_histogram(binwidth = 0.25, fill = "skyblue", color = "black") +
  labs(title = "Figure 4. Histogram of Predicted Readmission Rate for Hip/Knee Replacement",
       x = "Predicted Readmission Rate",
       y = "Frequency") +
  theme_minimal() + 
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank())
```

#### Creating a table of missing values
```{r}
# Calculate missing values
missing_values_summary <- HipKneeClean %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Missing_Count") %>%
  mutate(Missing_Percentage = (Missing_Count / nrow(HipKneeClean)) * 100)

# Print the table using kable
missing_values_summary %>%
  kable(caption = "Table 7. Missing Values Summary") %>%
  kable_styling(bootstrap_options = c("hover", "striped", "responsive"))
```

#### Assessing collinearity
```{r, fig.width=14, fig.height=12}
# Compute correlation matrix
cor_matrix <- cor(HipKneeClean %>% select_if(is.numeric), use = "pairwise.complete.obs")

# Melt the correlation matrix into a long format
cor_melted <- melt(cor_matrix)

# Plot the heatmap
ggplot(cor_melted, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Figure 5. Correlation Heatmap of Numeric Variables")

# Convert the correlation matrix to a data frame
cor_table <- as.data.frame(cor_matrix)

# Add variable names as a column for better readability
cor_table$Variable <- rownames(cor_table)

# Reorder columns for better readability
cor_table <- cor_table %>%
  select(Variable, everything())

# Print the table using kable
cor_table %>%
  kable(caption = "Table 8. Correlation Coefficients Table") %>%
  kable_styling(bootstrap_options = c("hover", "striped", "responsive"))
```

# Preprocessing

## Variable Encoding (SE)

#### Identify Categorical Variables
```{r}
# Create function to find categorical variables
is_categorical <- function(x) is.factor(x) | is.character(x)

# Apply function to all variables in the dataset
categorical_vars <- sapply(HipKneeClean, is_categorical)

# Print the names of all categorical variables
categorical <- names(HipKneeClean)[categorical_vars]
categorical
```

#### Dummy encode the EDV column 
```{r}
# Define the encoding mapping (ignore NAs for now)
encoding_map <- c(
  'low' = 1,
  'medium' = 2,
  'high' = 3,
  'very high' = 4
)
# Dummy encoding used due to ordinal nature of this data

# Create a copy of HipKneeClean and name it HipKneeTrain to separate cleaned dataset and the training dataset
HipKneeTrain <- HipKneeClean %>%
  mutate(EDV = recode(EDV, !!!encoding_map))

# Print first 20 rows of EDV column in HipKneeClean and HipKneeTrain to ensure proper encoding
cat("HipKneeClean")
print(head(HipKneeClean$EDV, 20))

cat("HipKneeTrain")
print(head(HipKneeTrain$EDV, 20))
```

#### Encode each state in alphabetical order
```{r}
# Manually map out each state with their respective code in alphabetical order with a preceding 0 to make data non-ordinal
state_mapping <- c(
  "AL" = "001",
  "AK" = "002",
  "AZ" = "003",
  "AR" = "004",
  "CA" = "005",
  "CO" = "006",
  "CT" = "007",
  "DE" = "008",
  "FL" = "009",
  "GA" = "010",
  "HI" = "011",
  "ID" = "012",
  "IL" = "013",
  "IN" = "014",
  "IA" = "015",
  "KS" = "016",
  "KY" = "017",
  "LA" = "018",
  "ME" = "019",
  "MD" = "020",
  "MA" = "021",
  "MI" = "022",
  "MN" = "023",
  "MS" = "024",
  "MO" = "025",
  "MT" = "026",
  "NE" = "027",
  "NV" = "028",
  "NH" = "029",
  "NJ" = "030",
  "NM" = "031",
  "NY" = "032",
  "NC" = "033",
  "ND" = "034",
  "OH" = "035",
  "OK" = "036",
  "OR" = "037",
  "PA" = "038",
  "RI" = "039",
  "SC" = "040",
  "SD" = "041",
  "TN" = "042",
  "TX" = "043",
  "UT" = "044",
  "VT" = "045",
  "VA" = "046",
  "WA" = "047",
  "WV" = "048",
  "WI" = "049",
  "WY" = "050"
)

# Create new "StateCode" column with the encoded values
HipKneeTrain <- HipKneeTrain %>%
  mutate(StateCode = state_mapping[State])

# Print 100 rows of the "State" and "StateCode" columns to ensure accuracy
print("State and StateCode Columns")
print(head(HipKneeTrain[c("State", "StateCode")], 100))

# Print all unique values in "StateCode" column to ensure accuracy
print("Unique StateCode Values")
print(unique(HipKneeTrain$StateCode))
```

## Collinearity and Feature Removal (SE)

#### Remove correlated and unnecessary variables 
```{r}
# Specify columns to remove
columns_to_remove <- c(
  "ED_2_Strata_1",
  "OP_23",
  "VTE_2",
  "OP_18c",
  "OP_22",
  "STK_02",
  "STK_05",
  "STK_06",
  "HcahpsLinearMeanValue_H_RECMND_LINEAR_SCORE",
  "NumberOfReadmissions_HIP-KNEE",
  "ExcessReadmissionRatio_HIP-KNEE",
  "ExpectedReadmissionRate_HIP-KNEE",
  "SEP_1",
  "SEV_SEP_6HR",
  "SEV_SEP_3HR",
  "SEP_SH_6HR",
  "SEP_SH_3HR",
  "Score_PSI_90",
  "PatientSurveyStarRating_H_COMP_1_STAR_RATING",    
  "PatientSurveyStarRating_H_COMP_2_STAR_RATING",    
  "PatientSurveyStarRating_H_COMP_3_STAR_RATING",    
  "PatientSurveyStarRating_H_COMP_5_STAR_RATING",    
  "PatientSurveyStarRating_H_COMP_6_STAR_RATING",    
  "PatientSurveyStarRating_H_COMP_7_STAR_RATING",    
  "PatientSurveyStarRating_H_CLEAN_STAR_RATING",     
  "PatientSurveyStarRating_H_QUIET_STAR_RATING",     
  "PatientSurveyStarRating_H_HSP_RATING_STAR_RATING",
  "PatientSurveyStarRating_H_RECMND_STAR_RATING",    
  "PatientSurveyStarRating_H_STAR_RATING",
  "HcahpsLinearMeanValue_H_COMP_1_LINEAR_SCORE",    
  "HcahpsLinearMeanValue_H_COMP_2_LINEAR_SCORE",    
  "HcahpsLinearMeanValue_H_COMP_3_LINEAR_SCORE",    
  "HcahpsLinearMeanValue_H_COMP_5_LINEAR_SCORE",    
  "HcahpsLinearMeanValue_H_COMP_6_LINEAR_SCORE",    
  "HcahpsLinearMeanValue_H_COMP_7_LINEAR_SCORE",    
  "HcahpsLinearMeanValue_H_CLEAN_LINEAR_SCORE",     
  "HcahpsLinearMeanValue_H_QUIET_LINEAR_SCORE"
)

# Remove specified columns
HipKneeTrain <- HipKneeTrain %>% select(-all_of(columns_to_remove))
```

```{r}
# Print column names to verify
print("Remaining columns:")
print(colnames(HipKneeTrain))
```
> "OP_18c" and "OP_22" removed due to being highly correlated and low relevance.
> "ED_2_Strata_1", "OP_23", and "VTE_2" removed due to high percentage of missingness. 
> "STK_02", "STK_05", and "STK_06" variables removed as we do not see stroke data as being relevant towards Hip/Knee Surgery.
> "HcahpsLinearMeanValue_H_RECMND_LINEAR_ SCORE" removed as it is strongly correlated with overall hospital rating.
> "ExcessReadmissionRatio_HIP-KNEE", and "ExpectedReadmissionRate_HIP-KNEE" removed due to being highly correlated with the target variable. "NumberOfReadmissions_HIP-KNEE" removed as this would be highly influenced by hospital size and we have no data on hospital sizes.
> Sepsis variables removed due to unclear definition in the dataset's dictionary of what the values represent.
> Score_PSI_90 variable removed because it's a summary of the other PSI variables. We chose to include all the individual PSI variables, which makes the summary variable redundant. 
> We chose to remove a lot of the patient survey data due to collinearity and redundancy. The average star rating data is redundant with the linear mean score data. We decided to keep the overall hospital rating linear mean score, and the hospital recommendation linear mean score columns. We felt that these variables summarized the other, more granular, metrics. For example COMP-1 is nurse responsiveness and COMP-2 is doctor responsiveness, COMP-3 is staff responsiveness. It makes sense that a lot of these were collinear. We had considered engineering the comp features (1-7) together into a single patient experience variable, however, this was collinear with overall hospital rating and recommendation. We also chose to go with the linear mean score overall hospital rating and recommendation score, because the dataset essentially already scaled these variables for us by performing a linear transformation. 
> It always seems a little scary removing entire chunks of variables, as we wouldn't want to miss any significant relationships between the variables. Do you think this is a wise decision? Are there any other ideas you could think of to engineer variables in a way to keep more of them? 

#### Reassess collinearity with heatmap and correlation matrix
```{r, fig.width=14, fig.height=15}
# Compute correlation matrix
cor_matrix <- cor(HipKneeTrain %>% select_if(is.numeric), use = "pairwise.complete.obs")

# Melt the correlation matrix into a long format
cor_melted <- melt(cor_matrix)

# Plot heatmap
ggplot(cor_melted, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Figure 5. Correlation Heatmap of Numeric Variables")

# Convert correlation matrix to df
cor_table <- as.data.frame(cor_matrix)

# Add variable names as a column
cor_table$Variable <- rownames(cor_table)

# Reorder columns
cor_table <- cor_table %>%
  select(Variable, everything())

# Print table
cor_table %>%
  kable(caption = "Table 8. Correlation Coefficients Table") %>%
  kable_styling(bootstrap_options = c("hover", "striped", "responsive"))
```

## Imputation and Handling of Missing Values (AC)

```{r}
# Remove all NA values in target variable "PredictedReadmissionRate_HIP_KNEE"
HipKneeTrain <- HipKneeTrain %>% filter(!is.na(PredictedReadmissionRate_HIP_KNEE))

# Remove all NA values in the "State", "StateCode", and "FacilityName" columns
HipKneeTrain <- HipKneeTrain %>% drop_na(State, StateCode, FacilityName)


# Print number of remaining variables and observations
dimensions <- dim(HipKneeTrain)
cat("Number of variables:", dimensions[2], "\n")
cat("Number of observations:", dimensions[1], "\n")
```
> We decided to remove the one facility that had an NA value, which also happened to be the same observation with a missing state value.

```{r}
# Calculate missing values
missing_values_summary <- HipKneeTrain %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Missing_Count") %>%
  mutate(Missing_Percentage = (Missing_Count / nrow(HipKneeTrain)) * 100)

# Print table
missing_values_summary %>%
  kable(caption = "Table 7. Missing Values Summary") %>%
  kable_styling(bootstrap_options = c("hover", "striped", "responsive"))
```
#### Impute variables with low percentage missingness (<5%) by the median for numeric variables 
```{r}
# Calculate median for columns with <5% missing values
numeric_vars_low_missing <- c("HcahpsLinearMeanValue_H_HSP_RATING_LINEAR_SCORE", "EDV", "HCP_COVID_19", "IMM_3", "OP_18b", "SAFE_USE_OF_OPIOIDS", "Score_COMP_HIP_KNEE", "Score_PSI_03", "Score_PSI_06", "Score_PSI_08", "Score_PSI_09", "Score_PSI_10", "Score_PSI_11", "Score_PSI_12", "Score_PSI_13", "Score_PSI_14", "Score_PSI_15", "Payment_PAYM_90_HIP_KNEE")

for (var in numeric_vars_low_missing) {
  HipKneeTrain[[var]][is.na(HipKneeTrain[[var]])] <- median(HipKneeTrain[[var]], na.rm = TRUE)
}
```

#### Impute high percentage missingness variables (>5%) using KNN
```{r}
# Select high missingness variables for KNN imputation
vars_for_knn <- c("VTE_1", "Score_MORT_30_AMI", "Score_MORT_30_COPD", "Score_MORT_30_HF", "Score_MORT_30_PN", "Score_MORT_30_STK", "Score_PSI_04", "OP_29")

# Perform KNN imputation
HipKneeTrain_knn <- kNN(HipKneeTrain, variable = vars_for_knn, k = 5)

# Remove columns created by the KNN function
HipKneeTrain_knn <- HipKneeTrain_knn %>% select(-ends_with("_imp"))

# Update HipKneeTrain with imputed values
HipKneeTrain[vars_for_knn] <- HipKneeTrain_knn[vars_for_knn]
```

> Is this a good method for imputing missing values? We decided that many of our variables had very low missingness percentage, <1%. So, Median imputation would be fine in this case. For the few variables that had higher missingness we went with KNN imputation. Do you have any suggestions or ideas that would be more appropriate here?

```{r}
# Calculate missing values
missing_values_summary <- HipKneeTrain %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Missing_Count") %>%
  mutate(Missing_Percentage = (Missing_Count / nrow(HipKneeTrain)) * 100)

# Print table
missing_values_summary %>%
  kable(caption = "Table 7. Missing Values Summary") %>%
  kable_styling(bootstrap_options = c("hover", "striped", "responsive"))
```

## Feature Engineer Mortality Data (AC)
```{r}
# Average death rates amongst mortality variables and create new column "Score_Ovr_MORT"
HipKneeTrain$Score_Ovr_MORT <- rowMeans(HipKneeTrain[, c("Score_MORT_30_AMI", 
                                                         "Score_MORT_30_COPD", 
                                                         "Score_MORT_30_HF", 
                                                         "Score_MORT_30_PN", 
                                                         "Score_MORT_30_STK")], 
                                                          na.rm = TRUE)

# Remove old mortality columns
HipKneeTrain <- HipKneeTrain[, !(names(HipKneeTrain) %in% c("Score_MORT_30_AMI", 
                                                            "Score_MORT_30_COPD",
                                                            "Score_MORT_30_HF", 
                                                            "Score_MORT_30_PN", 
                                                            "Score_MORT_30_STK"))]

```

#### Reassess heatmap with engineered mortality data
```{r, fig.width=14, fig.height=15}
# Compute correlation matrix
cor_matrix <- cor(HipKneeTrain %>% select_if(is.numeric), use = "pairwise.complete.obs")

# Melt the correlation matrix into a long format
cor_melted <- melt(cor_matrix)

# Plot heatmap
ggplot(cor_melted, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Figure 5. Correlation Heatmap of Numeric Variables")
```

## Preprocessing the test dataset (AC)
We are utilizing the most recent snapshot from 04/24/2024 as our test set. Utilizing this brand new data will help to ensure that our model is generalizable and useful for future analyses. 

### Loading the Data 
```{r, message=FALSE, warning=FALSE}
# Set the directory for the data files
filepath <- "/Users/adelinecasali/Desktop/hospitals_04_2024/" 

# List the files in the directory that have "Hospital.csv"
files <- list.files(path = filepath, pattern = "Hospital.csv")

# Iterate through each file in the list
for(f in 1:length(files)) {
  
# Read the CSV, clean column names to upper camel case, and store in "dat"
    dat <- clean_names(read_csv(paste0(filepath, files[f]),
                                show_col_types = FALSE), 
                       case = "upper_camel")
    
# Remove ".Hospital.csv" part of the file names to create variable name
    filename <- gsub(".Hospital\\.csv", "", files[f])
    
# Assign data to a variable with the above created name
    assign(filename, dat)
}
# Create a df of file names without ".Hospital.csv"
files <- gsub(".Hospital\\.csv", "", files) %>% data.frame()

# Set column name of the df to "File Name"
names(files) <- "File Name"

files %>% 
  kable(
    format = "html",
    caption = "Table 1. List of hospital-level data files.") %>%
    kable_styling(bootstrap_options = c("striped", full_width = F)
  )
```

### Exploring and Preprocessing the FY_2024_Hospital_Readmissions_Reduction_Program dataset (AC)  

#### Viewing and checking for missing values  
```{r}
# Display first 10 rows of FY_2024_Hospital_Readmissions_Reduction_Program 
head(FY_2024_Hospital_Readmissions_Reduction_Program,10)

# Filter dataset to include numeric columns only
num_vars <- FY_2024_Hospital_Readmissions_Reduction_Program %>%
  select_if(is.numeric)

# Check for missing values
miss_vals <- sapply(num_vars, function(x) sum(is.na(x)))
print(miss_vals)
```

#### Replacing values with NA and "Too Few to Report" values with "5"  
```{r}
# Use the function "replace_with_na_all()" to replace aberrant values with NA
FY_2024_Hospital_Readmissions_Reduction_Program <- replace_with_na_all(FY_2024_Hospital_Readmissions_Reduction_Program, condition = ~ .x == "N/A")

# Replace "Too Few to Report" values with "5" in using gsub
FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions <- gsub("Too Few to Report", "5", FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions)

# Check first 10 rows to confirm that it worked
head(FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions, 10)

# NumberOfReadmissions had to be converted to numeric before applying integers
FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions <- as.numeric(FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions)

# Find all values of "5" in NumberOfReadmissions
fives <- which(FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions == 5)

# Replace values of "5" with random integers from 1 - 10
FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions[fives] <- sample(1:10, length(fives), replace = TRUE)

# Check the first 20 rows to see if this was applied correctly
head(FY_2024_Hospital_Readmissions_Reduction_Program$NumberOfReadmissions,20)
```

#### Converting columns to numeric  
```{r}
# Selecting the columns to convert
columns_to_convert <- c("NumberOfDischarges", "ExcessReadmissionRatio", "PredictedReadmissionRate", "ExpectedReadmissionRate", "NumberOfReadmissions")

# Use mutate_at to convert the specified columns to numeric
FY_2024_Hospital_Readmissions_Reduction_Program <- FY_2024_Hospital_Readmissions_Reduction_Program %>%
  mutate_at(vars(one_of(columns_to_convert)), as.numeric)

# Print the structure of the dataframe to check the changes
str(FY_2024_Hospital_Readmissions_Reduction_Program)
```

#### Removing excess text from measure names
```{r}
FY_2024_Hospital_Readmissions_Reduction_Program <-  FY_2024_Hospital_Readmissions_Reduction_Program %>%
  mutate(MeasureName = gsub("READM-30-", "", MeasureName)) %>% 
  mutate(MeasureName = gsub("-HRRP", "", MeasureName)) 
```

#### Pivoting the data wider  
```{r}
readmissionsClean <- FY_2024_Hospital_Readmissions_Reduction_Program %>%
  pivot_wider(
    names_from = MeasureName, 
    values_from = c(NumberOfDischarges, ExcessReadmissionRatio, PredictedReadmissionRate, ExpectedReadmissionRate, NumberOfReadmissions), 
    id_cols = c(FacilityName, FacilityId, State, StartDate, EndDate)
  )

# Check the new dataframe
dim(readmissionsClean)
head(readmissionsClean)
```

#### Filtering for only hip/knee conditions
```{r}
readmissionsClean <- readmissionsClean %>%
  select(FacilityName, FacilityId, State, matches("HIP-KNEE$"))
```

### Exploring and Preprocessing the HCAHPS dataset 

#### Viewing and checking for missing values  
```{r}
# Display first 10 rows of HCAHPS 
head(HCAHPS,10)

# Filter dataset to include numeric columns only
num_vars <- HCAHPS %>%
  select_if(is.numeric)

# Check for missing values
miss_vals <- sapply(num_vars, function(x) sum(is.na(x)))
print(miss_vals)
```

#### Removing footnote columns and replacing NA values  
```{r}
# Removing all footnote columns
HCAHPS <- HCAHPS %>%
  select(-ends_with("footnote"))

# Replacing all "Not Applicable" with NA
HCAHPS <- as.data.frame(sapply(HCAHPS, function(x) {
  if (is.character(x)) {
    x[x == "Not Applicable"] <- NA
  }
  return(x)
}))

# Replacing all "Not Available" with NA
HCAHPS <- as.data.frame(sapply(HCAHPS, function(x) {
  if (is.character(x)) {
    x[x == "Not Available"] <- NA
  }
  return(x)
}))
```

#### Pivoting the data wider  
```{r}
HCAHPSClean <- HCAHPS %>%
  pivot_wider(
    names_from = HcahpsMeasureId, 
    values_from = c(PatientSurveyStarRating, HcahpsAnswerPercent, HcahpsLinearMeanValue, SurveyResponseRatePercent), 
    id_cols = c(FacilityName, FacilityId, State)
  )

# Check the new dataframe
dim(HCAHPSClean)
head(HCAHPSClean)
```

### Exploring and Preprocessing the Timely_and_Effective_Care dataset 

#### Viewing and checking for missing values  
```{r}
# Display first 10 rows of Timely_and_Effective_Care
head(Timely_and_Effective_Care,10)

# Filter dataset to include numeric columns only
num_vars <- Timely_and_Effective_Care %>%
  select_if(is.numeric)

# Check for missing values
miss_vals <- sapply(num_vars, function(x) sum(is.na(x)))
print(miss_vals)
```

#### Replacing NA values  
```{r}
# Replacing all "Not Applicable" with NA
Timely_and_Effective_Care <- as.data.frame(sapply(Timely_and_Effective_Care, function(x) {
  if (is.character(x)) {
    x[x == "Not Applicable"] <- NA
  }
  return(x)
}))

# Replacing all "Not Available" with NA
Timely_and_Effective_Care <- as.data.frame(sapply(Timely_and_Effective_Care, function(x) {
  if (is.character(x)) {
    x[x == "Not Available"] <- NA
  }
  return(x)
}))
```

#### Pivoting the data wider  
```{r}
careClean <- Timely_and_Effective_Care %>%
  pivot_wider(
    names_from = MeasureId, 
    values_from = c(Score), 
    id_cols = c(FacilityName, FacilityId, State)
  )

# Check the new dataframe
dim(careClean)
head(careClean)
```

### Exploring and Preprocessing the Complications_and_Deaths dataset

#### Viewing and checking for missing values  
```{r}
# Display first 10 rows of Complications_and_Deaths
head(Complications_and_Deaths,10)

# Filter dataset to include numeric columns only
num_vars <- Complications_and_Deaths %>%
  select_if(is.numeric)

# Check for missing values
miss_vals <- sapply(num_vars, function(x) sum(is.na(x)))
print(miss_vals)
```

#### Replacing NA values  
```{r}
# Replacing all "Not Applicable" with NA
Complications_and_Deaths <- as.data.frame(sapply(Complications_and_Deaths, function(x) {
  if (is.character(x)) {
    x[x == "Not Applicable"] <- NA
  }
  return(x)
}))

# Replacing all "Not Available" with NA
Complications_and_Deaths <- as.data.frame(sapply(Complications_and_Deaths, function(x) {
  if (is.character(x)) {
    x[x == "Not Available"] <- NA
  }
  return(x)
}))
```

#### Pivoting the data wider  
```{r}
deathsClean <- Complications_and_Deaths %>%
  pivot_wider(
    names_from = MeasureId, 
    values_from = c(ComparedToNational, Score), 
    id_cols = c(FacilityName, FacilityId, State)
  )

# Check the new dataframe
dim(deathsClean)
head(deathsClean)
```

### Exploring and Preprocessing the Payment_and_Value_of_Care dataset 

#### Viewing and checking for missing values  
```{r}
# Display first 10 rows of Payment_and_Value_of_Care
head(Payment_and_Value_of_Care,10)

# Filter dataset to include numeric columns only
num_vars <- Payment_and_Value_of_Care %>%
  select_if(is.numeric)

# Check for missing values
miss_vals <- sapply(num_vars, function(x) sum(is.na(x)))
print(miss_vals)
```

#### Replacing NA values  
```{r}
# Replacing all "Not Applicable" with NA
Payment_and_Value_of_Care <- as.data.frame(sapply(Payment_and_Value_of_Care, function(x) {
  if (is.character(x)) {
    x[x == "Not Applicable"] <- NA
  }
  return(x)
}))

# Replacing all "Not Available" with NA
Payment_and_Value_of_Care <- as.data.frame(sapply(Payment_and_Value_of_Care, function(x) {
  if (is.character(x)) {
    x[x == "Not Available"] <- NA
  }
  return(x)
}))
```

#### Pivoting the data wider  
```{r}
paymentClean <- Payment_and_Value_of_Care %>%
  pivot_wider(
    names_from = PaymentMeasureId, 
    values_from = c(PaymentCategory, Payment), 
    id_cols = c(FacilityName, FacilityId, State)
  )

# Check the new dataframe
dim(paymentClean)
head(paymentClean)
```

#### Selecting only Hip-Knee related  
```{r}
paymentClean <- paymentClean %>%
  select(FacilityName, FacilityId, State, matches("HIP_KNEE$"))
```

### Joining and cleaning the datasets  

#### Joining the datasets based on FacilityId
```{r}
HipKneeCleanTest <- readmissionsClean %>%
  full_join(HCAHPSClean, by = "FacilityId") %>%
  full_join(careClean, by = "FacilityId") %>%
  full_join(deathsClean, by = "FacilityId") %>%
  full_join(paymentClean, by = "FacilityId")

head(HipKneeCleanTest)
```

#### Removing redundant columns  
```{r}
# Removing duplicate columns
HipKneeCleanTest <- HipKneeCleanTest %>%
  select(-matches("\\.(x|y|z|w|v)$"))
```

#### Checking for NA Values  
```{r, results='hide'}
# Checking the dimensions
dim(HipKneeCleanTest)

# Count NA values in each column
na_counts <- sapply(HipKneeCleanTest, function(x) sum(is.na(x)))

# View the NA counts
print(na_counts)
```

#### Removing columns with more than 80% NA values
```{r, results='hide'}
# Calculate the percentage of NA values for each column
na_percentage <- sapply(HipKneeCleanTest, function(x) mean(is.na(x)))

# Remove columns where more than 80% of the values are NA
HipKneeCleanTest <- HipKneeCleanTest[, na_percentage <= 0.8]

# Count NA values in each column
na_counts <- sapply(HipKneeCleanTest, function(x) sum(is.na(x)))

# View the NA counts
print(na_counts)

# Check the dimensions
dim(HipKneeCleanTest)
```

#### Removing answer percent and survey response percent columns
```{r}
# Remove columns containing 'AnswerPercent' or 'SurveyResponseRate'
HipKneeCleanTest <- HipKneeCleanTest %>%
  select(-matches("AnswerPercent|SurveyResponseRate"))

# Check the dimensions
dim(HipKneeCleanTest)
```

#### Removing compared to national columns  
```{r}
# Remove columns containing 'ComparedToNational' and 'PaymentCategory'
HipKneeCleanTest <- HipKneeCleanTest %>%
  select(-matches("ComparedToNational|PaymentCategory"))

# Check the dimensions
dim(HipKneeCleanTest)
```

#### Checking data structure
```{r}
str(HipKneeCleanTest)

# Convert columns to numeric
HipKneeCleanTest <- HipKneeCleanTest %>%
  mutate_at(vars(starts_with("PatientSurveyStarRating_"), 
                 starts_with("HcahpsLinearMeanValue_"), 
                 starts_with("Score_"),
                 starts_with("ED_"),
                 starts_with("IMM_"),
                 starts_with("OP_"),
                 starts_with("SEP_"),
                 starts_with("SEV_"),
                 starts_with("STK_"),
                 starts_with("VTE_"),
                 starts_with("SAFE_"),
                 starts_with("HCP_")),
            ~ as.numeric(as.character(.)))

# View the structure
str(HipKneeCleanTest)
```

#### Fixing the payment column
```{r}
# Remove $ and , and convert to numeric
HipKneeCleanTest <- HipKneeCleanTest %>%
  mutate_at(vars(starts_with("Payment_")), 
            ~ as.numeric(gsub("[\\$,]", "", .)))

# Checking the structure
str(HipKneeCleanTest)
```

### Encoding categorical variables

#### Identify Categorical Variables
```{r}
# Create function to find categorical variables
is_categorical <- function(x) is.factor(x) | is.character(x)

# Apply function to all variables in the dataset
categorical_vars <- sapply(HipKneeClean, is_categorical)

# Print the names of all categorical variables
categorical <- names(HipKneeClean)[categorical_vars]
categorical
```

#### Dummy encode the EDV column 
```{r}
# Define the encoding mapping (ignore NAs for now)
encoding_map <- c(
  'low' = 1,
  'medium' = 2,
  'high' = 3,
  'very high' = 4
)
# Dummy encoding used due to ordinal nature of this data

# Create a copy of HipKneeCleanTest and name it HipKneeTest to separate cleaned dataset and the test dataset
HipKneeTest <- HipKneeCleanTest %>%
  mutate(EDV = recode(EDV, !!!encoding_map))

# Print first 20 rows of EDV column in HipKneeClean and HipKneeTrain to ensure proper encoding
cat("HipKneeCleanTest")
print(head(HipKneeCleanTest$EDV, 20))

cat("HipKneeTest")
print(head(HipKneeTest$EDV, 20))
```

#### Encode each state in alphabetical order
```{r}
# Manually map out each state with their respective code in alphabetical order with a preceding 0 to make data non-ordinal
state_mapping <- c(
  "AL" = "001",
  "AK" = "002",
  "AZ" = "003",
  "AR" = "004",
  "CA" = "005",
  "CO" = "006",
  "CT" = "007",
  "DE" = "008",
  "FL" = "009",
  "GA" = "010",
  "HI" = "011",
  "ID" = "012",
  "IL" = "013",
  "IN" = "014",
  "IA" = "015",
  "KS" = "016",
  "KY" = "017",
  "LA" = "018",
  "ME" = "019",
  "MD" = "020",
  "MA" = "021",
  "MI" = "022",
  "MN" = "023",
  "MS" = "024",
  "MO" = "025",
  "MT" = "026",
  "NE" = "027",
  "NV" = "028",
  "NH" = "029",
  "NJ" = "030",
  "NM" = "031",
  "NY" = "032",
  "NC" = "033",
  "ND" = "034",
  "OH" = "035",
  "OK" = "036",
  "OR" = "037",
  "PA" = "038",
  "RI" = "039",
  "SC" = "040",
  "SD" = "041",
  "TN" = "042",
  "TX" = "043",
  "UT" = "044",
  "VT" = "045",
  "VA" = "046",
  "WA" = "047",
  "WV" = "048",
  "WI" = "049",
  "WY" = "050"
)

# Create new "StateCode" column with the encoded values
HipKneeTest <- HipKneeTest %>%
  mutate(StateCode = state_mapping[State])

# Print 100 rows of the "State" and "StateCode" columns to ensure accuracy
print("State and StateCode Columns")
print(head(HipKneeTest[c("State", "StateCode")], 100))

# Print all unique values in "StateCode" column to ensure accuracy
print("Unique StateCode Values")
print(unique(HipKneeTest$StateCode))
```

### Collinearity and Feature Removal

#### Remove correlated and unnecessary variables 
```{r}
# Specify columns to remove
columns_to_remove <- c(
  "ED_2_Strata_1",
  "OP_23",
  "VTE_2",
  "OP_18c",
  "OP_22",
  "STK_02",
  "STK_05",
  "STK_06",
  "HcahpsLinearMeanValue_H_RECMND_LINEAR_SCORE",
  "NumberOfReadmissions_HIP-KNEE",
  "ExcessReadmissionRatio_HIP-KNEE",
  "ExpectedReadmissionRate_HIP-KNEE",
  "SEP_1",
  "SEV_SEP_6HR",
  "SEV_SEP_3HR",
  "SEP_SH_6HR",
  "SEP_SH_3HR",
  "Score_PSI_90",
  "PatientSurveyStarRating_H_COMP_1_STAR_RATING",    
  "PatientSurveyStarRating_H_COMP_2_STAR_RATING",    
  "PatientSurveyStarRating_H_COMP_3_STAR_RATING",    
  "PatientSurveyStarRating_H_COMP_5_STAR_RATING",    
  "PatientSurveyStarRating_H_COMP_6_STAR_RATING",    
  "PatientSurveyStarRating_H_COMP_7_STAR_RATING",    
  "PatientSurveyStarRating_H_CLEAN_STAR_RATING",     
  "PatientSurveyStarRating_H_QUIET_STAR_RATING",     
  "PatientSurveyStarRating_H_HSP_RATING_STAR_RATING",
  "PatientSurveyStarRating_H_RECMND_STAR_RATING",    
  "PatientSurveyStarRating_H_STAR_RATING",
  "HcahpsLinearMeanValue_H_COMP_1_LINEAR_SCORE",    
  "HcahpsLinearMeanValue_H_COMP_2_LINEAR_SCORE",    
  "HcahpsLinearMeanValue_H_COMP_3_LINEAR_SCORE",    
  "HcahpsLinearMeanValue_H_COMP_5_LINEAR_SCORE",    
  "HcahpsLinearMeanValue_H_COMP_6_LINEAR_SCORE",    
  "HcahpsLinearMeanValue_H_COMP_7_LINEAR_SCORE",    
  "HcahpsLinearMeanValue_H_CLEAN_LINEAR_SCORE",     
  "HcahpsLinearMeanValue_H_QUIET_LINEAR_SCORE"
)

# Remove specified columns
HipKneeTest <- HipKneeTest %>% select(-all_of(columns_to_remove))
```

```{r}
# Print column names to verify
print("Remaining columns:")
print(colnames(HipKneeTest))
```

#### Reassess collinearity with heatmap and correlation matrix
```{r, fig.width=14, fig.height=15}
# Compute correlation matrix
cor_matrix <- cor(HipKneeTest %>% select_if(is.numeric), use = "pairwise.complete.obs")

# Melt the correlation matrix into a long format
cor_melted <- melt(cor_matrix)

# Plot heatmap
ggplot(cor_melted, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Figure 5. Correlation Heatmap of Numeric Variables")

# Convert correlation matrix to df
cor_table <- as.data.frame(cor_matrix)

# Add variable names as a column
cor_table$Variable <- rownames(cor_table)

# Reorder columns
cor_table <- cor_table %>%
  select(Variable, everything())

# Print table
cor_table %>%
  kable(caption = "Table 8. Correlation Coefficients Table") %>%
  kable_styling(bootstrap_options = c("hover", "striped", "responsive"))
```

### Imputation and Handling of Missing Values

```{r}
# Change - to _ in HIP-KNEE
colnames(HipKneeTest) <- gsub("-", "_", colnames(HipKneeTest))

# Remove all NA values in target variable "PredictedReadmissionRate_HIP_KNEE"
HipKneeTest <- HipKneeTest %>% filter(!is.na(PredictedReadmissionRate_HIP_KNEE))

# Remove all NA values in the "State", "StateCode", and "FacilityName" columns
HipKneeTest <- HipKneeTest %>% drop_na(State, StateCode, FacilityName)


# Print number of remaining variables and observations
dimensions <- dim(HipKneeTest)
cat("Number of variables:", dimensions[2], "\n")
cat("Number of observations:", dimensions[1], "\n")
```

```{r}
# Calculate missing values
missing_values_summary <- HipKneeTest %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Missing_Count") %>%
  mutate(Missing_Percentage = (Missing_Count / nrow(HipKneeTest)) * 100)

# Print table
missing_values_summary %>%
  kable(caption = "Table 7. Missing Values Summary") %>%
  kable_styling(bootstrap_options = c("hover", "striped", "responsive"))
```

#### Impute variables with low percentage missingness (<5%) by the median for numeric variables and mode for categorical variables
```{r}
# Calculate median for columns with <5% missing values
numeric_vars_low_missing <- c("HcahpsLinearMeanValue_H_HSP_RATING_LINEAR_SCORE", "EDV", "HCP_COVID_19", "IMM_3", "OP_18b", "SAFE_USE_OF_OPIOIDS", "Score_COMP_HIP_KNEE", "Score_PSI_03", "Score_PSI_06", "Score_PSI_08", "Score_PSI_09", "Score_PSI_10", "Score_PSI_11", "Score_PSI_12", "Score_PSI_13", "Score_PSI_14", "Score_PSI_15", "Payment_PAYM_90_HIP_KNEE")

for (var in numeric_vars_low_missing) {
  HipKneeTest[[var]][is.na(HipKneeTest[[var]])] <- median(HipKneeTest[[var]], na.rm = TRUE)
}
```

#### Impute high percentage missingness variables (>5%) using KNN
```{r}
# Select high missingness variables for KNN imputation
vars_for_knn <- c("VTE_1", "Score_MORT_30_AMI", "Score_MORT_30_COPD", "Score_MORT_30_HF", "Score_MORT_30_PN", "Score_MORT_30_STK", "Score_PSI_04", "OP_29")

# Perform KNN imputation
HipKneeTest_knn <- kNN(HipKneeTest, variable = vars_for_knn, k = 5)

# Remove columns created by the KNN function
HipKneeTest_knn <- HipKneeTest_knn %>% select(-ends_with("_imp"))

# Update HipKneeTrain with imputed values
HipKneeTest[vars_for_knn] <- HipKneeTest_knn[vars_for_knn]
```

```{r}
# Calculate missing values
missing_values_summary <- HipKneeTest %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Missing_Count") %>%
  mutate(Missing_Percentage = (Missing_Count / nrow(HipKneeTest)) * 100)

# Print table
missing_values_summary %>%
  kable(caption = "Table 7. Missing Values Summary") %>%
  kable_styling(bootstrap_options = c("hover", "striped", "responsive"))
```

#### Feature Engineer Mortality Data 
```{r}
# Average death rates amongst mortality variables and create new column "Score_Ovr_MORT"
HipKneeTest$Score_Ovr_MORT <- rowMeans(HipKneeTest[, c("Score_MORT_30_AMI", 
                                                         "Score_MORT_30_COPD", 
                                                         "Score_MORT_30_HF", 
                                                         "Score_MORT_30_PN", 
                                                         "Score_MORT_30_STK")], 
                                                          na.rm = TRUE)

# Remove old mortality columns
HipKneeTest <- HipKneeTest[, !(names(HipKneeTest) %in% c("Score_MORT_30_AMI", 
                                                            "Score_MORT_30_COPD",
                                                            "Score_MORT_30_HF", 
                                                            "Score_MORT_30_PN", 
                                                            "Score_MORT_30_STK"))]

```

#### Reassess heatmap with engineered mortality data
```{r, fig.width=14, fig.height=15}
# Compute correlation matrix
cor_matrix <- cor(HipKneeTest %>% select_if(is.numeric), use = "pairwise.complete.obs")

# Melt the correlation matrix into a long format
cor_melted <- melt(cor_matrix)

# Plot heatmap
ggplot(cor_melted, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Figure 5. Correlation Heatmap of Numeric Variables")
```

#### Save the data for future ease of use
```{r}
save(HipKneeTest, file = "HipKneeTest.RData")
```

## Descriptive Statistics (AC)
```{r}
# Create a summary table of descriptive statistics
descr_stats <- describe(HipKneeTrain)
# Remove the rows with Facility ID, State and State code, and facility name
descr_stats <- descr_stats %>% filter(vars != c(1, 23, 24, 26))
# Remove columns 1, 2, 5, and 6
descr_stats <- descr_stats[, -c(1, 2, 5, 6)]

# Create a table with kable
kable(descr_stats, format = "html", caption = "Descriptive Statistics for All Numeric Variables in Final Dataset") %>%
  kable_styling(
    bootstrap_options = c("hover", "striped", "responsive")
  ) %>%
  column_spec(1, bold = TRUE) %>%
  column_spec(2, width = "5em") %>%
  row_spec(0, bold = TRUE, background = "#f2f2f2")

# Select numeric columns
numeric_columns <- HipKneeTrain %>% select_if(is.numeric)

# Melt the data for easier plotting with ggplot2
numeric_melted <- melt(numeric_columns)

# Create histograms
ggplot(numeric_melted, aes(x = value)) +
  geom_histogram(bins = 30, fill = "blue", color = "black") +
  facet_wrap(~variable, scales = "free_x") +
  theme_minimal() +
  labs(title = "Histograms of Numeric Variables", x = "Value", y = "Frequency")
```


# Segmentation Analysis  

## k-means clustering (SE)
```{r}
# Select numeric columns for clustering
numeric_columns <- HipKneeTrain %>% select_if(is.numeric)

# Standardize features
X_scaled <- scale(numeric_columns)

# Determine optimal number of clusters using elbow plot
set.seed(123)
elbow_plot <- fviz_nbclust(X_scaled, kmeans, method = "wss", k.max = 10) +
  labs(title = "Elbow Plot for Optimal k")

print(elbow_plot)

# Optimal K = 3
optimal_k <- 3
kmeans_result <- kmeans(X_scaled, centers = optimal_k, nstart = 25)

# Create a new df for K-Means Clustering results
HipKneeTrain_K_Means <- HipKneeTrain %>%
  mutate(Cluster = as.factor(kmeans_result$cluster))

# Visualize clusters
fviz_cluster(kmeans_result, data = X_scaled,
             ellipse.type = "convex",
             palette = "jco",
             ggtheme = theme_minimal())

# Cluster characteristics
cluster_summary <- HipKneeTrain_K_Means %>%
  group_by(Cluster) %>%
  summarise_if(is.numeric, mean, na.rm = TRUE)

print(cluster_summary)

# Visualize feature distributions across clusters
features_to_plot <- c("PredictedReadmissionRate_HIP_KNEE", "HcahpsLinearMeanValue_H_HSP_RATING_LINEAR_SCORE", "Score_COMP_HIP_KNEE", "SAFE_USE_OF_OPIOIDS")

for (feature in features_to_plot) {
  p <- ggplot(HipKneeTrain_K_Means, aes(x = Cluster, y = .data[[feature]], fill = Cluster)) +
    geom_boxplot() +
    theme_minimal() +
    labs(title = paste("Distribution of", feature, "across clusters"))
  print(p)
}
```

## Hierarchical Clustering (SE)
```{r}
# Perform PCA
pca_result <- prcomp(X_scaled, center = TRUE, scale. = TRUE)

# Visualize variance
fviz_eig(pca_result, addlabels = TRUE)

# Factor map
fviz_pca_var(pca_result, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE)

# PC scores, three components
pca_scores <- as.data.frame(pca_result$x[, 1:3])

# Store PCA results in a new dataframe
HipKneeTrain_PCA <- HipKneeTrain %>%
  select_if(is.numeric) %>%
  bind_cols(pca_scores)

# Hierarchical Clustering

# Compute distance matrix
dist_matrix <- dist(X_scaled, method = "euclidean")

# Perform hierarchical clustering
hc_result <- hclust(dist_matrix, method = "ward.D2")

# Compute WCSS for different number of clusters
wcss <- sapply(1:10, function(k) {
  clusters <- cutree(hc_result, k)
  cluster_data <- scale(X_scaled)
  tot.withinss <- sum(sapply(unique(clusters), function(c) {
    sum(dist(cluster_data[clusters == c, , drop = FALSE])^2)
  }))
  return(tot.withinss)
})

# Plot WCSS
plot(1:10, wcss, type = "b", xlab = "Number of Clusters", ylab = "WCSS")

# Create clusters with optimal number of clusters from WCSS plot
k <- 3
hc_clusters <- cutree(hc_result, k = k)

# Store hierarchical clustering results in a new dataframe
HipKneeTrain_HC <- HipKneeTrain %>%
  mutate(HC_Cluster = as.factor(hc_clusters))

# Visualize clusters using first three PCs
pca_plot_data <- cbind(pca_scores[, 1:3], Cluster = hc_clusters)
fviz_cluster(list(data = pca_plot_data, cluster = hc_clusters),
             ellipse.type = "convex",
             palette = "jco",
             ggtheme = theme_minimal(),
             main = "Hierarchical Clustering Results (PCA)")

# Analyze cluster characteristics
hc_cluster_summary <- HipKneeTrain_HC %>%
  group_by(HC_Cluster) %>%
  summarise_if(is.numeric, mean, na.rm = TRUE)

print(hc_cluster_summary)

# Visualize feature distributions across clusters
features_to_plot <- c("PredictedReadmissionRate_HIP_KNEE", "HcahpsLinearMeanValue_H_HSP_RATING_LINEAR_SCORE", "Score_COMP_HIP_KNEE", "SAFE_USE_OF_OPIOIDS")

for (feature in features_to_plot) {
  p <- ggplot(HipKneeTrain_HC, aes_string(x = "HC_Cluster", y = feature, fill = "HC_Cluster")) +
    geom_boxplot() +
    theme_minimal() +
    labs(title = paste("Distribution of", feature, "across Hierarchical Clusters"))
  print(p)
}
```
> This is a preliminary segmentation analysis and we would go forth and tighten/tidy this up some more. However, just initial impression, I'm not sure clustering is entirely beneficial with our dataset. What are your thoughts on our preliminary segmentation analysis? Any ideas in which we could improve our clustering to perhaps be more meaningful?

# Supervised Modeling with Quantitative Variable

## Random Forest (SE)

### Creating the model
```{r}
# Remove unwanted columns from the dataset
HipKneeTrain_RF <- HipKneeTrain %>%
  select(-State, -FacilityName, -FacilityId)

# Define mtry parameter grid
grid <- expand.grid(
  mtry = c(2, 4, 6, 8)
)

# Define CV
train_control <- trainControl(
  method = "cv",         
  number = 7,           
  verboseIter = TRUE     
)

# Train the Random Forest model with grid search
rf_grid_search <- train(
  PredictedReadmissionRate_HIP_KNEE ~ .,   
  data = HipKneeTrain_RF,                    
  method = "rf",                          
  trControl = train_control,              
  tuneGrid = grid,                       
  importance = TRUE,                     
  ntree = 100                         
)

# Best parameters
best_params <- rf_grid_search$bestTune
print(best_params)

# Extract feature importances
best_rf_model <- rf_grid_search$finalModel
feature_importances <- importance(best_rf_model)

# Convert feature importances to a df
feature_importances_df <- as.data.frame(feature_importances)
feature_importances_df$Feature <- rownames(feature_importances_df)

# Sort importances by %IncMSE
sorted_by_inc_mse <- feature_importances_df %>%
  arrange(desc(`%IncMSE`))

# Sort importances by IncNodePurity
sorted_by_inc_node_purity <- feature_importances_df %>%
  arrange(desc(IncNodePurity))

# Print importances
cat("Feature Importances by %IncMSE:\n")
print(sorted_by_inc_mse)

cat("\nFeature Importances by IncNodePurity:\n")
print(sorted_by_inc_node_purity)
```

### Assessing Random Forest Performance
```{r}
# Remove columns from the test set to match train set
HipKneeTest_RF <- HipKneeTest %>%
  select(-State, -FacilityName, -FacilityId)

# Make predictions on test set
rf_predictions <- predict(rf_grid_search, newdata = HipKneeTest_RF)

# Actual values
actual_values <- HipKneeTest$PredictedReadmissionRate_HIP_KNEE

# Calculate RMSE
mse <- mean((rf_predictions - actual_values)^2)
rmse <- sqrt(mse)

# Calculate R-squared
ss_total <- sum((actual_values - mean(actual_values))^2)
ss_residual <- sum((rf_predictions - actual_values)^2)
r_squared <- 1 - (ss_residual / ss_total)

# Print RMSE and R-squared
cat("RMSE on test set:\n")
print(rmse)

cat("\nR-squared on test set:\n")
print(r_squared)
```

### Testing assumptions
```{r}
# Calculate residuals
residuals_rf <- actual_values - rf_predictions

# Residuals vs Fitted Values plot
ggplot(data = NULL, aes(x = rf_predictions, y = residuals_rf)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "loess", se = FALSE, color = "blue") +
  labs(title = "Residuals vs Fitted Values",
       x = "Fitted Values",
       y = "Residuals") +
  theme_minimal()

# Histogram of residuals
ggplot(data = NULL, aes(x = residuals_rf)) +
  geom_histogram(binwidth = 0.1, fill = "blue", alpha = 0.7, boundary = 0) +
  labs(title = "Histogram of Residuals",
       x = "Residuals",
       y = "Frequency") +
  theme_minimal()

# QQ plot of residuals
qqnorm(residuals_rf, main = "QQ Plot of Residuals")
qqline(residuals_rf, col = "red")

# Perform Durbin-Watson test for autocorrelation in residuals
dw_test_result <- dwtest(lm(residuals_rf ~ rf_predictions))
print(dw_test_result)
```

## Elastic Net (AC)

### Creating the model
```{r}
# Separate predictors and response variable in the training set
x_train <- as.matrix(HipKneeTrain %>% select(-c(State, FacilityName, PredictedReadmissionRate_HIP_KNEE)))
y_train <- HipKneeTrain$PredictedReadmissionRate_HIP_KNEE

# Separate predictors and response variable in the test set
x_test <- as.matrix(HipKneeTest %>% select(-c(State, FacilityName, PredictedReadmissionRate_HIP_KNEE)))
y_test <- HipKneeTest$PredictedReadmissionRate_HIP_KNEE

# Define the grid of hyperparameters
searchGrid <- expand.grid(.alpha = seq(0, 1, length.out = 10), 
                          .lambda = seq(0, 5, length.out = 15))

# Define the train control
ctrl <- trainControl(method = "repeatedcv", 
                     number = 10, 
                     repeats = 5, 
                     search = "grid", 
                     verboseIter = FALSE)

# Set up cross-validation
elasticnet_model <- train(
  x = x_train,
  y = y_train,
  method = "glmnet",
  trControl = ctrl,
  tuneGrid = searchGrid
)

# Best hyperparameters
best_alpha <- elasticnet_model$bestTune$alpha
best_lambda <- elasticnet_model$bestTune$lambda

# Print best alpha and lambda
print(paste("Best Alpha: ", best_alpha))
print(paste("Best Lambda: ", best_lambda))
```

### Assessing performance
```{r}
# Make predictions on the test set
predictions <- predict(elasticnet_model, newdata = x_test)

# Calculate RMSE
rmse <- sqrt(mean((predictions - y_test)^2))

# Print RMSE
print(paste("RMSE on Test Set: ", rmse))

# Calculate performance metrics on the test set
performance <- postResample(pred = predictions, obs = y_test)

# Extract and print R-squared
r_squared <- performance["Rsquared"]
print(paste("R^2 on Test Set: ", r_squared))

# Get the feature importance
important <- varImp(elasticnet_model)$importance

# View the feature importance
important %>% 
  mutate(Feature = rownames(important)) %>% 
  mutate(Feature = gsub("\\.", " ", Feature)) %>% 
  arrange(desc(Overall)) %>% 
  ggplot(aes(y = Overall, fill = Overall, x = fct_reorder(Feature, Overall))) +
  geom_col() + 
  scale_fill_continuous(low = "lightblue", high = "darkblue") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Feature importance as determined by Elastic Net",
       x = "",
       y = "Importance", 
       fill = "")
```


## SVM (AC)

### Creating the model
#### Selecting the kernel
```{r}
# Convert x_train and x_test back to data frames
x_train <- as.data.frame(x_train)
x_test <- as.data.frame(x_test)

# Ensure all columns are numeric
x_train[] <- lapply(x_train, as.numeric)
x_test[] <- lapply(x_test, as.numeric)

# Convert y_train to a data frame
train_data <- cbind(y_train = y_train, x_train)

# Define the grid for kernel types
searchGrid_kernel <- expand.grid(
  .kernel = c("linear", "polynomial", "radial", "sigmoid")
)

# Train the SVM model with kernel tuning
svm_tune_kernel <- tune(svm, 
                        y_train ~ ., 
                        data = train_data, 
                        ranges = searchGrid_kernel, 
                        tunecontrol = tune.control(
                          sampling = "cross", 
                          cross = 10
                        )
)

# Extract the best kernel
best_kernel <- svm_tune_kernel$best.model$kernel
if (best_kernel == 0) {
  kernel_description <- "Linear kernel"
} else if (best_kernel == 1) {
  kernel_description <- "Polynomial kernel"
} else if (best_kernel == 2) {
  kernel_description <- "Radial kernel"
} else if (best_kernel == 3) {
  kernel_description <- "Sigmoid kernel"
} else {
  kernel_description <- "Unknown kernel"
}

cat("Best Kernel Description:", kernel_description, "\n")
```

#### Tuning gamma and cost
```{r}
# Define the grid for gamma
searchGrid_gamma <- expand.grid(
  gamma = c(0.01, 0.1, 1)
)

# Train the SVM model with gamma tuning
svm_tune_gamma <- tune(svm, 
                       y_train ~ ., 
                       data = train_data, 
                       ranges = searchGrid_gamma, 
                       kernel = "radial", 
                       tunecontrol = tune.control(
                         sampling = "cross", 
                         cross = 10
                       )
)

# Extract the best gamma
best_gamma <- svm_tune_gamma$best.model$gamma
cat("Best Gamma:", best_gamma, "\n")

# Define the grid for cost
searchGrid_cost <- expand.grid(
  C = c(0.1, 1, 10)
)

# Train the SVM model with cost tuning
svm_tune_cost <- tune(svm, 
                      y_train ~ ., 
                      data = train_data, 
                      ranges = searchGrid_cost, 
                      kernel = "radial", 
                      tunecontrol = tune.control(
                        sampling = "cross", 
                        cross = 10
                      )
)

# Extract the best cost
best_cost <- svm_tune_cost$best.model$cost
cat("Best Cost:", best_cost, "\n")
```

#### Create the final model
```{r}
# Final model with best parameters
svm_final <- svm(y_train ~ ., 
                  data = train_data,
                  kernel = "radial", 
                  C = 1, 
                  gamma = 0.01,
                  probability = TRUE)
```

### Assessing performance
```{r}
# Make predictions on the test set
predictions <- predict(svm_final, x_test, type = "response")

# Calculate RMSE
rmse <- sqrt(mean((predictions - y_test)^2))
cat("RMSE on Test Set:", rmse, "\n")

# Calculate R-squared
rss <- sum((y_test - predictions)^2)
tss <- sum((y_test - mean(y_test))^2)
r_squared <- 1 - (rss / tss)
cat("R-squared on Test Set:", r_squared, "\n")
```

# Supervised Modeling with Qualitative Variable

## Categorizing the target variable (AC)
```{r}
# Check for median
print(median(HipKneeTrain$PredictedReadmissionRate_HIP_KNEE, na.rm = TRUE))
print(median(HipKneeTest$PredictedReadmissionRate_HIP_KNEE, na.rm = TRUE))

# Calculate the median of the target variable from the training data
median_value <- median(HipKneeTrain$PredictedReadmissionRate_HIP_KNEE, na.rm = TRUE)

# Categorize the target variable in the training data
HipKneeTrain_Qual <- HipKneeTrain %>%
  mutate(TargetCategory = ifelse(PredictedReadmissionRate_HIP_KNEE > median_value, 1, 0))

# Categorize the target variable in the testing data using the median from the training data
HipKneeTest_Qual <- HipKneeTest %>%
  mutate(TargetCategory = ifelse(PredictedReadmissionRate_HIP_KNEE > median_value, 1, 0))
```

## Random Forest (AC)

### Create the model
```{r}
# Remove unwanted columns from the dataset
HipKneeTrain_QualRF <- HipKneeTrain_Qual %>%
  select(-State, -FacilityName, -FacilityId, -PredictedReadmissionRate_HIP_KNEE)

# Define mtry parameter grid
grid <- expand.grid(
  mtry = c(2, 4, 6, 8)
)

# Define CV
train_control <- trainControl(
  method = "cv",         
  number = 7,           
  verboseIter = TRUE     
)

# Ensure the target variable is a factor
HipKneeTrain_QualRF$TargetCategory <- as.factor(HipKneeTrain_QualRF$TargetCategory)

# Train the Random Forest model with grid search
rf_grid_search_qual <- train(
  TargetCategory ~ .,   
  data = HipKneeTrain_QualRF,                    
  method = "rf",                          
  trControl = train_control,              
  tuneGrid = grid,                       
  importance = TRUE,                     
  ntree = 100                         
)

# Best parameters
best_params_qual <- rf_grid_search_qual$bestTune
print(best_params_qual)

# Extract feature importances
best_rf_model_qual <- rf_grid_search_qual$finalModel
feature_importances_qual <- importance(best_rf_model_qual)

# Convert feature importances to a df
feature_importances_df_qual <- as.data.frame(feature_importances_qual)
feature_importances_df_qual$Feature <- rownames(feature_importances_df_qual)

# Rename columns for clarity
colnames(feature_importances_df_qual) <- c("MeanDecreaseAccuracy", "MeanDecreaseGini", "Feature")

# Sort by MeanDecreaseGini
sorted_by_importance_qual <- feature_importances_df_qual[order(-feature_importances_df_qual$MeanDecreaseGini), ]

# Print the sorted table
print(sorted_by_importance_qual)
```

### Assessing the performance
```{r}
# Remove columns from the test set to match train set
HipKneeTest_QualRF <- HipKneeTest_Qual %>%
  select(-State, -FacilityName, -FacilityId, -PredictedReadmissionRate_HIP_KNEE)

# Ensure the target variable is a factor
HipKneeTest_QualRF$TargetCategory <- as.factor(HipKneeTest_QualRF$TargetCategory)

# Predict on the test set
pred_rf_qual <- predict(rf_grid_search_qual, newdata = HipKneeTest_QualRF)

# Calculate accuracy
accuracy_rf_qual <- mean(pred_rf_qual == HipKneeTest_QualRF$TargetCategory)
cat("Accuracy of the Random Forest Model:", accuracy_rf_qual, "\n")

# Calculate the confusion matrix
conf_matrix_rf_qual <- confusionMatrix(pred_rf_qual, HipKneeTest_QualRF$TargetCategory)

# Print the confusion matrix
print(conf_matrix_rf_qual)

# Predict on the test set
pred_rf_qual <- predict(rf_grid_search_qual, newdata = HipKneeTest_QualRF, type = "prob")

# Extract the predicted probabilities for the positive class (assuming "1" is the positive class)
pred_prob_pos <- pred_rf_qual[, "1"]

# Generate predictions object for ROCR
pred <- prediction(pred_prob_pos, HipKneeTest_QualRF$TargetCategory)

# Calculate and plot ROC curve
roc_curve <- performance(pred, "tpr", "fpr")
plot(roc_curve, main = "ROC Curve for Random Forest Model", col = "blue")

# Calculate AUC
auc <- performance(pred, "auc")@y.values[[1]]
print(paste("AUC:", auc))
```

## Logistic Regression (AC)

### Creating the model
```{r}
# Ensure factors have consistent levels between training and test sets
factor_columns <- sapply(HipKneeTrain_Qual, is.factor)

HipKneeTest_Qual <- HipKneeTest_Qual %>%
  mutate(across(where(is.factor), ~ factor(.x, levels = levels(HipKneeTrain_Qual[[cur_column()]]))))

# Separate predictors and response variable in the training set
x_train_qual <- HipKneeTrain_Qual %>% select(-c(State, FacilityName, FacilityId, PredictedReadmissionRate_HIP_KNEE, TargetCategory))
y_train_qual <- HipKneeTrain_Qual$TargetCategory

# Ensure the response variable is a factor
y_train_qual <- as.factor(y_train_qual)

# Separate predictors and response variable in the test set
x_test_qual <- HipKneeTest_Qual %>% select(-c(State, FacilityName, FacilityId, PredictedReadmissionRate_HIP_KNEE, TargetCategory))
y_test_qual <- HipKneeTest_Qual$TargetCategory

# Ensure the response variable is a factor
y_test_qual <- as.factor(y_test_qual)

# Define the train control for logistic regression
ctrl <- trainControl(method = "repeatedcv", 
                     number = 10, 
                     repeats = 5, 
                     verboseIter = FALSE)

# Set up cross-validation for logistic regression
logistic_model_qual <- train(
  x = x_train_qual,
  y = y_train_qual,
  method = "glm",
  trControl = ctrl,
  family = "binomial"
)

# Print the model summary
best_model <- logistic_model_qual$finalModel
summary(best_model)
```

### Assessing performance
```{r}
# Predict class labels on the test set based on a threshold 
predicted_labels <- ifelse(predictions > 0.5, 1, 0) 

# Convert predicted labels to factors with the same levels as y_test_qual
predicted_labels <- factor(predicted_labels, levels = levels(y_test_qual))

# Calculate accuracy
accuracy <- mean(predicted_labels == y_test_qual)
print(paste("Accuracy: ", accuracy))

# Create a confusion matrix
conf_matrix <- caret::confusionMatrix(predicted_labels, y_test_qual)
print(conf_matrix)

# ROC and AUC
roc_curve <- roc(y_test_qual, predictions)
auc <- auc(roc_curve)
print(paste("AUC: ", auc))

# Plot ROC curve
plot(roc_curve, main = "ROC Curve for Logistic Regression", col = "blue")

# Extract the coefficients from the best model
coefficients <- coef(logistic_model_qual$finalModel, s = best_lambda_qual)

# Convert coefficients to a dataframe
coefficients_df <- as.data.frame(as.matrix(coefficients))
coefficients_df$Feature <- rownames(coefficients_df)
coefficients_df <- coefficients_df %>% 
  mutate(Feature = gsub("\\.", " ", Feature)) %>% 
  rename(Importance = V1) %>%
  arrange(desc(abs(Importance))) %>%
  filter(Feature != "(Intercept)")  # Remove the intercept term if present

# Plot feature importance
ggplot(coefficients_df, aes(y = Importance, fill = Importance, x = fct_reorder(Feature, Importance))) +
  geom_col() + 
  scale_fill_continuous(low = "lightblue", high = "darkblue") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Feature Importance as Determined by Logistic Regression",
       x = "",
       y = "Coefficient Value", 
       fill = "Coefficient")
```

## SVM (AC)

### Creating the model
#### Selecting the kernel
```{r}
# Convert x_train and x_test back to data frames
x_train_qual <- as.data.frame(x_train_qual)
x_test_qual <- as.data.frame(x_test_qual)

# Ensure all columns are numeric
x_train_qual[] <- lapply(x_train_qual, as.numeric)
x_test_qual[] <- lapply(x_test_qual, as.numeric)

# Convert y_train to a data frame
train_data_qual <- cbind(y_train_qual = y_train_qual, x_train_qual)

# Define the grid for kernel types
searchGrid_kernel <- expand.grid(
  .kernel = c("linear", "polynomial", "radial", "sigmoid")
)

# Train the SVM model with kernel tuning
svm_tune_kernel_qual <- tune(svm, 
                        y_train_qual ~ ., 
                        data = train_data_qual, 
                        ranges = searchGrid_kernel, 
                        tunecontrol = tune.control(
                          sampling = "cross", 
                          cross = 10
                        )
)

# Extract the best kernel
best_kernel_qual <- svm_tune_kernel_qual$best.model$kernel
if (best_kernel_qual == 0) {
  kernel_description_qual <- "Linear kernel"
} else if (best_kernel_qual == 1) {
  kernel_description_qual <- "Polynomial kernel"
} else if (best_kernel_qual == 2) {
  kernel_description_qual <- "Radial kernel"
} else if (best_kernel_qual == 3) {
  kernel_description_qual <- "Sigmoid kernel"
} else {
  kernel_description_qual <- "Unknown kernel"
}

cat("Best Kernel Description:", kernel_description_qual, "\n")
```

#### Tuning gamma and cost
```{r}
# Define the grid for gamma
searchGrid_gamma <- expand.grid(
  gamma = c(0.01, 0.1, 1)
)

# Train the SVM model with gamma tuning
svm_tune_gamma_qual <- tune(svm, 
                       y_train_qual ~ ., 
                       data = train_data_qual, 
                       ranges = searchGrid_gamma, 
                       kernel = "radial", 
                       tunecontrol = tune.control(
                         sampling = "cross", 
                         cross = 10
                       )
)

# Extract the best gamma
best_gamma_qual <- svm_tune_gamma_qual$best.model$gamma
cat("Best Gamma:", best_gamma_qual, "\n")

# Define the grid for cost
searchGrid_cost <- expand.grid(
  C = c(0.1, 1, 10)
)

# Train the SVM model with cost tuning
svm_tune_cost_qual <- tune(svm, 
                      y_train_qual ~ ., 
                      data = train_data_qual, 
                      ranges = searchGrid_cost, 
                      kernel = "radial", 
                      tunecontrol = tune.control(
                        sampling = "cross", 
                        cross = 10
                      )
)

# Extract the best cost
best_cost_qual <- svm_tune_cost_qual$best.model$cost
cat("Best Cost:", best_cost_qual, "\n")
```

#### Create the final model
```{r}
# Final model with best parameters
svm_final_qual <- svm(y_train_qual ~ ., 
                  data = train_data_qual,
                  kernel = "radial", 
                  C = 1, 
                  gamma = 0.01,
                  probability = TRUE)
```

### Assessing performance
```{r}
# Predict on the test set
pred_test_svm_tuned <- predict(svm_final_qual, x_test_qual, probability = TRUE)

# Create the confusion matrix
confMat_tuned <- caret::confusionMatrix(pred_test_svm_tuned, y_test_qual)
print(confMat_tuned)

# Extract decision values for ROC curve
decision_values <- attributes(predict(svm_final_qual, x_test_qual, decision.values = TRUE))$decision.values
pred <- prediction(decision_values, y_test_qual)
roc_curve <- performance(pred, "tpr", "fpr")

# Plot ROC curve
plot(roc_curve, main = "ROC Curve for Tuned SVM", col = "blue")

# Calculate AUC
pred_prob <- attr(predict(svm_final_qual, x_test_qual, probability = TRUE), "probabilities")[, c(0, 1)]
auc_value <- roc(y_test_qual, pred_prob)$auc
print(paste("AUC:", auc_value))
```

## Neural Network (SE)

```{r}
# Load tensorflow (loaded here due to package conflictions)
pacman::p_load(tensorflow)

# Calculate readmission rate national median
national_median <- median(HipKneeTrain_Qual$PredictedReadmissionRate_HIP_KNEE)

# Create a copy of the HipKneeTest_Qual and add PreferredStatus column to copied df
HipKneeTest_Qual_nn <- HipKneeTest_Qual %>%
  mutate(PreferredStatus = ifelse(PredictedReadmissionRate_HIP_KNEE > national_median, "Preferred", "Non-Preferred"))

# Make sure all variables are numeric
HipKneeTrain_Qual <- HipKneeTrain_Qual %>%
  mutate(across(where(is.character), as.factor)) %>%
  mutate(across(where(is.factor), as.numeric))

HipKneeTest_Qual <- HipKneeTest_Qual %>%
  mutate(across(where(is.character), as.factor)) %>%
  mutate(across(where(is.factor), as.numeric))

# Prepare features and target
x_train <- HipKneeTrain_Qual %>%
  # Remove unwanted columns from train data
  select(-State, -FacilityName, -TargetCategory, -FacilityId, -PredictedReadmissionRate_HIP_KNEE) %>%
  as.matrix()

y_train <- ifelse(HipKneeTrain_Qual$PredictedReadmissionRate_HIP_KNEE > national_median, 1, 0)

x_test <- HipKneeTest_Qual %>%
  # Remove unwanted columns from test data
  select(-State, -FacilityName, -FacilityId, -TargetCategory, -PredictedReadmissionRate_HIP_KNEE) %>%
  as.matrix()

y_test <- ifelse(HipKneeTest_Qual$PredictedReadmissionRate_HIP_KNEE > national_median, 1, 0)

# Convert target variable back to categorical
y_train <- keras::to_categorical(y_train, num_classes = 2)
y_test <- keras::to_categorical(y_test, num_classes = 2)

# Define function to create and compile model
create_model <- function(units1 = 64, units2 = 32, dropout_rate = 0.5, learning_rate = 0.001) {
  model <- keras_model_sequential() %>%
    layer_dense(units = units1, activation = 'relu', input_shape = dim(x_train)[2]) %>%
    layer_dropout(rate = dropout_rate) %>%
    layer_dense(units = units2, activation = 'relu') %>%
    layer_dropout(rate = dropout_rate) %>%
    layer_dense(units = 2, activation = 'softmax')
  
  model %>% compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizer_adam(learning_rate = learning_rate),
    metrics = c('accuracy')
  )
  
  return(model)
}

# Define function to train and evaluate model
evaluate_model <- function(params) {
  model <- create_model(
    units1 = params$units1,
    units2 = params$units2,
    dropout_rate = params$dropout_rate,
    learning_rate = params$learning_rate
  )
  
  history <- model %>% fit(
    x_train, y_train,
    epochs = 20,
    batch_size = 32,
    validation_split = 0.2,
    verbose = 0
  )
  
  accuracy <- max(history$metrics$val_accuracy)
  
  return(tibble(params, accuracy = accuracy))
}

# Define grid of hyperparameters
grid <- expand.grid(
  units1 = c(32, 64, 128),
  units2 = c(16, 32, 64),
  dropout_rate = c(0.3, 0.5, 0.7),
  learning_rate = c(0.001, 0.01, 0.1)
)

# Perform Grid Search
results <- bind_rows(lapply(seq_len(nrow(grid)), function(i) {
  params <- grid[i, ]
  evaluate_model(params)
}))

best_params <- results %>%
  arrange(desc(accuracy)) %>%
  slice(1)

print(best_params)

# Train the best model with the optimal parameters
best_model <- create_model(
  units1 = best_params$units1,
  units2 = best_params$units2,
  dropout_rate = best_params$dropout_rate,
  learning_rate = best_params$learning_rate
)

history <- best_model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_split = 0.2,
  verbose = 2
)

# Make predictions on test set
predictions <- best_model %>% predict(x_test)
predicted_classes <- max.col(predictions) - 1

# Add predictions to HipKneeTest_Qual_nn PredictedStatus column
HipKneeTest_Qual_nn$PredictedStatus <- ifelse(predicted_classes == 1, "Preferred", "Non-Preferred")

# Calculate Accuracy
accuracy <- sum(predicted_classes == (y_test[, 1])) / length(y_test[, 1])
cat("NN Test Accuracy:", accuracy, "\n")

# Calculate and print confusion matrix
confusion_matrix <- table(Predicted = predicted_classes, Actual = y_test[, 1])
print("NN Confusion Matrix:")
print(confusion_matrix)

# Print updated df with preferred/non-preferred predictions
print("Dataframe with NN Predictions:")
print(head(HipKneeTest_Qual_nn))
```

